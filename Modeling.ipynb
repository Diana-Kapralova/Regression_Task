{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Regression on the tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have a dataset (`internship_train.csv`) that contains **53 anonymized features and a target column**. Your task is to build model that predicts a target based on the proposed features. Please provide predictions for `internship_hidden_test.csv` file. Target metric is **RMSE**. The main goal is to provide **github repository** that contains:\n",
    "* jupyter notebook with analysis; \n",
    "* code for modeling (Python 3); \n",
    "* file with model predictions; \n",
    "* readme file;\n",
    "* requirements.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7F4D1DAE9940"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "sklearn.utils.check_random_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/DATA/diakap/Quantum_Test/Linear_Regression_Task'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working dir is where file located\n",
    "os.chdir(os.path.dirname(os.path.abspath('__file__')))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find test and train files\n",
    "for root, dirs, files  in os.walk(os.getcwd()):\n",
    "    for f in files:\n",
    "        if 'train' in f:\n",
    "            PATH_TRAIN = os.path.abspath(os.path.abspath(root + '/' + f))\n",
    "        elif 'hidden_test' in f:\n",
    "            PATH_TEST = os.path.abspath(os.path.abspath(root + '/' + f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Upload Train/Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>488</td>\n",
       "      <td>16</td>\n",
       "      <td>221</td>\n",
       "      <td>382</td>\n",
       "      <td>97</td>\n",
       "      <td>-4.472136</td>\n",
       "      <td>0.107472</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>281</td>\n",
       "      <td>336</td>\n",
       "      <td>99</td>\n",
       "      <td>3.880098</td>\n",
       "      <td>1.797502</td>\n",
       "      <td>3.252475</td>\n",
       "      <td>12.131981</td>\n",
       "      <td>3.091361</td>\n",
       "      <td>5.630319</td>\n",
       "      <td>4.466373</td>\n",
       "      <td>2.511203</td>\n",
       "      <td>5.982724</td>\n",
       "      <td>4.541159</td>\n",
       "      <td>12.740476</td>\n",
       "      <td>12.634929</td>\n",
       "      <td>4.050294</td>\n",
       "      <td>11.827245</td>\n",
       "      <td>3.568321</td>\n",
       "      <td>13.420537</td>\n",
       "      <td>8.251807</td>\n",
       "      <td>2.287900</td>\n",
       "      <td>14.834430</td>\n",
       "      <td>0.082253</td>\n",
       "      <td>2.975561</td>\n",
       "      <td>5.223753</td>\n",
       "      <td>1.212287</td>\n",
       "      <td>7.302797</td>\n",
       "      <td>7.083149</td>\n",
       "      <td>3.610350</td>\n",
       "      <td>7.767512</td>\n",
       "      <td>7.829657</td>\n",
       "      <td>8.395356</td>\n",
       "      <td>1.583711</td>\n",
       "      <td>10.125020</td>\n",
       "      <td>13.340874</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>1.962937</td>\n",
       "      <td>7.466666</td>\n",
       "      <td>11.547794</td>\n",
       "      <td>8.822916</td>\n",
       "      <td>9.046424</td>\n",
       "      <td>7.895535</td>\n",
       "      <td>11.010677</td>\n",
       "      <td>20.107472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>206</td>\n",
       "      <td>357</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>7.810250</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>109</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>12.099770</td>\n",
       "      <td>10.670550</td>\n",
       "      <td>14.137111</td>\n",
       "      <td>0.217037</td>\n",
       "      <td>1.426881</td>\n",
       "      <td>0.916617</td>\n",
       "      <td>8.168254</td>\n",
       "      <td>0.432319</td>\n",
       "      <td>5.872218</td>\n",
       "      <td>10.401401</td>\n",
       "      <td>12.843301</td>\n",
       "      <td>7.529992</td>\n",
       "      <td>12.525335</td>\n",
       "      <td>8.655344</td>\n",
       "      <td>8.499587</td>\n",
       "      <td>5.602552</td>\n",
       "      <td>4.187738</td>\n",
       "      <td>13.205982</td>\n",
       "      <td>5.432667</td>\n",
       "      <td>12.379275</td>\n",
       "      <td>11.938420</td>\n",
       "      <td>6.057282</td>\n",
       "      <td>2.581280</td>\n",
       "      <td>11.785456</td>\n",
       "      <td>4.445564</td>\n",
       "      <td>11.141346</td>\n",
       "      <td>0.839731</td>\n",
       "      <td>1.481908</td>\n",
       "      <td>8.920653</td>\n",
       "      <td>4.450379</td>\n",
       "      <td>10.584802</td>\n",
       "      <td>12.484882</td>\n",
       "      <td>7.168680</td>\n",
       "      <td>2.885415</td>\n",
       "      <td>12.413973</td>\n",
       "      <td>10.260494</td>\n",
       "      <td>10.091351</td>\n",
       "      <td>9.270888</td>\n",
       "      <td>3.173994</td>\n",
       "      <td>13.921871</td>\n",
       "      <td>61.763713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429</td>\n",
       "      <td>49</td>\n",
       "      <td>481</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>146</td>\n",
       "      <td>8.602325</td>\n",
       "      <td>0.651162</td>\n",
       "      <td>1</td>\n",
       "      <td>430</td>\n",
       "      <td>488</td>\n",
       "      <td>138</td>\n",
       "      <td>80</td>\n",
       "      <td>2.947479</td>\n",
       "      <td>12.671352</td>\n",
       "      <td>13.279918</td>\n",
       "      <td>13.163338</td>\n",
       "      <td>9.051826</td>\n",
       "      <td>11.605822</td>\n",
       "      <td>11.094161</td>\n",
       "      <td>10.461813</td>\n",
       "      <td>9.827713</td>\n",
       "      <td>2.206019</td>\n",
       "      <td>9.914789</td>\n",
       "      <td>4.448482</td>\n",
       "      <td>8.864810</td>\n",
       "      <td>10.837476</td>\n",
       "      <td>14.167872</td>\n",
       "      <td>13.456857</td>\n",
       "      <td>14.855511</td>\n",
       "      <td>7.596095</td>\n",
       "      <td>4.928033</td>\n",
       "      <td>2.439930</td>\n",
       "      <td>6.791165</td>\n",
       "      <td>8.709536</td>\n",
       "      <td>1.363087</td>\n",
       "      <td>4.980975</td>\n",
       "      <td>2.182372</td>\n",
       "      <td>14.673614</td>\n",
       "      <td>8.083289</td>\n",
       "      <td>6.601472</td>\n",
       "      <td>6.789200</td>\n",
       "      <td>12.982035</td>\n",
       "      <td>10.273114</td>\n",
       "      <td>14.030257</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>8.160625</td>\n",
       "      <td>12.592059</td>\n",
       "      <td>8.937577</td>\n",
       "      <td>2.265191</td>\n",
       "      <td>11.255721</td>\n",
       "      <td>12.794841</td>\n",
       "      <td>12.080951</td>\n",
       "      <td>74.651162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414</td>\n",
       "      <td>350</td>\n",
       "      <td>481</td>\n",
       "      <td>370</td>\n",
       "      <td>208</td>\n",
       "      <td>158</td>\n",
       "      <td>8.306624</td>\n",
       "      <td>0.424645</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>111</td>\n",
       "      <td>38</td>\n",
       "      <td>177</td>\n",
       "      <td>5.368252</td>\n",
       "      <td>6.765946</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>7.175573</td>\n",
       "      <td>14.515096</td>\n",
       "      <td>1.901743</td>\n",
       "      <td>9.231263</td>\n",
       "      <td>9.600810</td>\n",
       "      <td>10.058844</td>\n",
       "      <td>5.680021</td>\n",
       "      <td>8.238473</td>\n",
       "      <td>0.272760</td>\n",
       "      <td>11.892743</td>\n",
       "      <td>4.030567</td>\n",
       "      <td>14.946749</td>\n",
       "      <td>7.121632</td>\n",
       "      <td>6.507572</td>\n",
       "      <td>12.462688</td>\n",
       "      <td>12.222522</td>\n",
       "      <td>0.318528</td>\n",
       "      <td>5.350321</td>\n",
       "      <td>3.143358</td>\n",
       "      <td>10.291804</td>\n",
       "      <td>13.105170</td>\n",
       "      <td>10.159100</td>\n",
       "      <td>3.671488</td>\n",
       "      <td>11.087198</td>\n",
       "      <td>1.289054</td>\n",
       "      <td>0.249375</td>\n",
       "      <td>2.967133</td>\n",
       "      <td>6.885179</td>\n",
       "      <td>2.789577</td>\n",
       "      <td>6.416708</td>\n",
       "      <td>10.549814</td>\n",
       "      <td>11.456437</td>\n",
       "      <td>6.468099</td>\n",
       "      <td>2.519049</td>\n",
       "      <td>0.258284</td>\n",
       "      <td>9.317696</td>\n",
       "      <td>5.383098</td>\n",
       "      <td>69.424645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>359</td>\n",
       "      <td>20</td>\n",
       "      <td>218</td>\n",
       "      <td>317</td>\n",
       "      <td>301</td>\n",
       "      <td>8.124038</td>\n",
       "      <td>0.767304</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>141</td>\n",
       "      <td>417</td>\n",
       "      <td>343</td>\n",
       "      <td>14.592218</td>\n",
       "      <td>6.483629</td>\n",
       "      <td>9.159313</td>\n",
       "      <td>5.083046</td>\n",
       "      <td>8.772015</td>\n",
       "      <td>7.687242</td>\n",
       "      <td>11.106926</td>\n",
       "      <td>13.926358</td>\n",
       "      <td>0.914295</td>\n",
       "      <td>1.418124</td>\n",
       "      <td>4.504223</td>\n",
       "      <td>6.158475</td>\n",
       "      <td>1.790923</td>\n",
       "      <td>7.049614</td>\n",
       "      <td>14.409808</td>\n",
       "      <td>11.615837</td>\n",
       "      <td>5.675790</td>\n",
       "      <td>9.136146</td>\n",
       "      <td>10.640432</td>\n",
       "      <td>14.051122</td>\n",
       "      <td>2.240417</td>\n",
       "      <td>3.271828</td>\n",
       "      <td>5.061455</td>\n",
       "      <td>3.679880</td>\n",
       "      <td>2.948615</td>\n",
       "      <td>6.419577</td>\n",
       "      <td>14.873100</td>\n",
       "      <td>14.806887</td>\n",
       "      <td>9.992545</td>\n",
       "      <td>3.701959</td>\n",
       "      <td>11.162686</td>\n",
       "      <td>1.886560</td>\n",
       "      <td>1.919999</td>\n",
       "      <td>2.268203</td>\n",
       "      <td>0.149421</td>\n",
       "      <td>4.105907</td>\n",
       "      <td>10.416291</td>\n",
       "      <td>6.816217</td>\n",
       "      <td>8.586960</td>\n",
       "      <td>4.512419</td>\n",
       "      <td>66.767304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7  8    9   10   11   12  \\\n",
       "0  236  488   16  221  382   97 -4.472136  0.107472  0  132  281  336   99   \n",
       "1  386  206  357  232    1  198  7.810250  0.763713  1  143  109  123  130   \n",
       "2  429   49  481  111  111  146  8.602325  0.651162  1  430  488  138   80   \n",
       "3  414  350  481  370  208  158  8.306624  0.424645  1  340  111   38  177   \n",
       "4  318  359   20  218  317  301  8.124038  0.767304  1  212  141  417  343   \n",
       "\n",
       "          13         14         15         16         17         18  \\\n",
       "0   3.880098   1.797502   3.252475  12.131981   3.091361   5.630319   \n",
       "1  12.099770  10.670550  14.137111   0.217037   1.426881   0.916617   \n",
       "2   2.947479  12.671352  13.279918  13.163338   9.051826  11.605822   \n",
       "3   5.368252   6.765946   0.544415   7.175573  14.515096   1.901743   \n",
       "4  14.592218   6.483629   9.159313   5.083046   8.772015   7.687242   \n",
       "\n",
       "          19         20         21         22         23         24  \\\n",
       "0   4.466373   2.511203   5.982724   4.541159  12.740476  12.634929   \n",
       "1   8.168254   0.432319   5.872218  10.401401  12.843301   7.529992   \n",
       "2  11.094161  10.461813   9.827713   2.206019   9.914789   4.448482   \n",
       "3   9.231263   9.600810  10.058844   5.680021   8.238473   0.272760   \n",
       "4  11.106926  13.926358   0.914295   1.418124   4.504223   6.158475   \n",
       "\n",
       "          25         26         27         28         29         30  \\\n",
       "0   4.050294  11.827245   3.568321  13.420537   8.251807   2.287900   \n",
       "1  12.525335   8.655344   8.499587   5.602552   4.187738  13.205982   \n",
       "2   8.864810  10.837476  14.167872  13.456857  14.855511   7.596095   \n",
       "3  11.892743   4.030567  14.946749   7.121632   6.507572  12.462688   \n",
       "4   1.790923   7.049614  14.409808  11.615837   5.675790   9.136146   \n",
       "\n",
       "          31         32         33        34         35         36         37  \\\n",
       "0  14.834430   0.082253   2.975561  5.223753   1.212287   7.302797   7.083149   \n",
       "1   5.432667  12.379275  11.938420  6.057282   2.581280  11.785456   4.445564   \n",
       "2   4.928033   2.439930   6.791165  8.709536   1.363087   4.980975   2.182372   \n",
       "3  12.222522   0.318528   5.350321  3.143358  10.291804  13.105170  10.159100   \n",
       "4  10.640432  14.051122   2.240417  3.271828   5.061455   3.679880   2.948615   \n",
       "\n",
       "          38         39         40        41         42         43         44  \\\n",
       "0   3.610350   7.767512   7.829657  8.395356   1.583711  10.125020  13.340874   \n",
       "1  11.141346   0.839731   1.481908  8.920653   4.450379  10.584802  12.484882   \n",
       "2  14.673614   8.083289   6.601472  6.789200  12.982035  10.273114  14.030257   \n",
       "3   3.671488  11.087198   1.289054  0.249375   2.967133   6.885179   2.789577   \n",
       "4   6.419577  14.873100  14.806887  9.992545   3.701959  11.162686   1.886560   \n",
       "\n",
       "         45         46         47         48         49         50         51  \\\n",
       "0  0.870542   1.962937   7.466666  11.547794   8.822916   9.046424   7.895535   \n",
       "1  7.168680   2.885415  12.413973  10.260494  10.091351   9.270888   3.173994   \n",
       "2  0.394970   8.160625  12.592059   8.937577   2.265191  11.255721  12.794841   \n",
       "3  6.416708  10.549814  11.456437   6.468099   2.519049   0.258284   9.317696   \n",
       "4  1.919999   2.268203   0.149421   4.105907  10.416291   6.816217   8.586960   \n",
       "\n",
       "          52     target  \n",
       "0  11.010677  20.107472  \n",
       "1  13.921871  61.763713  \n",
       "2  12.080951  74.651162  \n",
       "3   5.383098  69.424645  \n",
       "4   4.512419  66.767304  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "dataset = pd.read_csv(PATH_TRAIN)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to analysis in 3.1 notebook we have 2 features that correlated between each other, so in future we try several combination with it to check model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preprocessing\n",
    "In this example use MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_columns = list(dataset.columns)\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation all columns ok. We scale target value also.\n",
    "scaling_feature = dataset_columns\n",
    "dataset_scaling = scaler.fit_transform(dataset[scaling_feature])\n",
    "dataset_scaling = pd.DataFrame(dataset_scaling)\n",
    "dataset_scaling.columns = dataset_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling All Features: Sklearn LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "\n",
    "Use this method, because if we use `train_test_split()` this means our program *sees* our test data, that not good for idea of hidden test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data on X and y\n",
    "X_data = dataset_scaling.loc[:, dataset_scaling.columns!='target']\n",
    "y_data = dataset_scaling['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.28908\n",
      "RMSE scores: [0.29004 0.2882  0.29009 0.28758 0.28949]\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]\n",
       "Mean_RMSE                                              0.28908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe for comparision all methods\n",
    "result_df = pd. DataFrame({'LinearRegression':[np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]},\n",
    "                         index=['RMSE_Cross_Val_5', 'Mean_RMSE'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling All Features: Sklearn MLPRegressor()\n",
    "Use Linear Reggression with 2 hidden layers. we use cross_val for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.02843\n",
      "RMSE scores: [0.03118 0.02748 0.03421 0.02324 0.02604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use MLPClassifier\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=10, random_state=1)\n",
    "rmse_scores = cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error')\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['MLPRegressor_50_relu'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]  \n",
       "Mean_RMSE                                               0.02843  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Delete feature 6 \n",
    "Feature 6 - that one feature, which has hole in histogram, but itself more smooth version of binary feature 8. Has bigger correlation with target, but correlation less then 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>488</td>\n",
       "      <td>16</td>\n",
       "      <td>221</td>\n",
       "      <td>382</td>\n",
       "      <td>97</td>\n",
       "      <td>-4.472136</td>\n",
       "      <td>0.107472</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>281</td>\n",
       "      <td>336</td>\n",
       "      <td>99</td>\n",
       "      <td>3.880098</td>\n",
       "      <td>1.797502</td>\n",
       "      <td>3.252475</td>\n",
       "      <td>12.131981</td>\n",
       "      <td>3.091361</td>\n",
       "      <td>5.630319</td>\n",
       "      <td>4.466373</td>\n",
       "      <td>2.511203</td>\n",
       "      <td>5.982724</td>\n",
       "      <td>4.541159</td>\n",
       "      <td>12.740476</td>\n",
       "      <td>12.634929</td>\n",
       "      <td>4.050294</td>\n",
       "      <td>11.827245</td>\n",
       "      <td>3.568321</td>\n",
       "      <td>13.420537</td>\n",
       "      <td>8.251807</td>\n",
       "      <td>2.287900</td>\n",
       "      <td>14.834430</td>\n",
       "      <td>0.082253</td>\n",
       "      <td>2.975561</td>\n",
       "      <td>5.223753</td>\n",
       "      <td>1.212287</td>\n",
       "      <td>7.302797</td>\n",
       "      <td>7.083149</td>\n",
       "      <td>3.610350</td>\n",
       "      <td>7.767512</td>\n",
       "      <td>7.829657</td>\n",
       "      <td>8.395356</td>\n",
       "      <td>1.583711</td>\n",
       "      <td>10.125020</td>\n",
       "      <td>13.340874</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>1.962937</td>\n",
       "      <td>7.466666</td>\n",
       "      <td>11.547794</td>\n",
       "      <td>8.822916</td>\n",
       "      <td>9.046424</td>\n",
       "      <td>7.895535</td>\n",
       "      <td>11.010677</td>\n",
       "      <td>20.107472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>206</td>\n",
       "      <td>357</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>7.810250</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>109</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>12.099770</td>\n",
       "      <td>10.670550</td>\n",
       "      <td>14.137111</td>\n",
       "      <td>0.217037</td>\n",
       "      <td>1.426881</td>\n",
       "      <td>0.916617</td>\n",
       "      <td>8.168254</td>\n",
       "      <td>0.432319</td>\n",
       "      <td>5.872218</td>\n",
       "      <td>10.401401</td>\n",
       "      <td>12.843301</td>\n",
       "      <td>7.529992</td>\n",
       "      <td>12.525335</td>\n",
       "      <td>8.655344</td>\n",
       "      <td>8.499587</td>\n",
       "      <td>5.602552</td>\n",
       "      <td>4.187738</td>\n",
       "      <td>13.205982</td>\n",
       "      <td>5.432667</td>\n",
       "      <td>12.379275</td>\n",
       "      <td>11.938420</td>\n",
       "      <td>6.057282</td>\n",
       "      <td>2.581280</td>\n",
       "      <td>11.785456</td>\n",
       "      <td>4.445564</td>\n",
       "      <td>11.141346</td>\n",
       "      <td>0.839731</td>\n",
       "      <td>1.481908</td>\n",
       "      <td>8.920653</td>\n",
       "      <td>4.450379</td>\n",
       "      <td>10.584802</td>\n",
       "      <td>12.484882</td>\n",
       "      <td>7.168680</td>\n",
       "      <td>2.885415</td>\n",
       "      <td>12.413973</td>\n",
       "      <td>10.260494</td>\n",
       "      <td>10.091351</td>\n",
       "      <td>9.270888</td>\n",
       "      <td>3.173994</td>\n",
       "      <td>13.921871</td>\n",
       "      <td>61.763713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429</td>\n",
       "      <td>49</td>\n",
       "      <td>481</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>146</td>\n",
       "      <td>8.602325</td>\n",
       "      <td>0.651162</td>\n",
       "      <td>1</td>\n",
       "      <td>430</td>\n",
       "      <td>488</td>\n",
       "      <td>138</td>\n",
       "      <td>80</td>\n",
       "      <td>2.947479</td>\n",
       "      <td>12.671352</td>\n",
       "      <td>13.279918</td>\n",
       "      <td>13.163338</td>\n",
       "      <td>9.051826</td>\n",
       "      <td>11.605822</td>\n",
       "      <td>11.094161</td>\n",
       "      <td>10.461813</td>\n",
       "      <td>9.827713</td>\n",
       "      <td>2.206019</td>\n",
       "      <td>9.914789</td>\n",
       "      <td>4.448482</td>\n",
       "      <td>8.864810</td>\n",
       "      <td>10.837476</td>\n",
       "      <td>14.167872</td>\n",
       "      <td>13.456857</td>\n",
       "      <td>14.855511</td>\n",
       "      <td>7.596095</td>\n",
       "      <td>4.928033</td>\n",
       "      <td>2.439930</td>\n",
       "      <td>6.791165</td>\n",
       "      <td>8.709536</td>\n",
       "      <td>1.363087</td>\n",
       "      <td>4.980975</td>\n",
       "      <td>2.182372</td>\n",
       "      <td>14.673614</td>\n",
       "      <td>8.083289</td>\n",
       "      <td>6.601472</td>\n",
       "      <td>6.789200</td>\n",
       "      <td>12.982035</td>\n",
       "      <td>10.273114</td>\n",
       "      <td>14.030257</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>8.160625</td>\n",
       "      <td>12.592059</td>\n",
       "      <td>8.937577</td>\n",
       "      <td>2.265191</td>\n",
       "      <td>11.255721</td>\n",
       "      <td>12.794841</td>\n",
       "      <td>12.080951</td>\n",
       "      <td>74.651162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414</td>\n",
       "      <td>350</td>\n",
       "      <td>481</td>\n",
       "      <td>370</td>\n",
       "      <td>208</td>\n",
       "      <td>158</td>\n",
       "      <td>8.306624</td>\n",
       "      <td>0.424645</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>111</td>\n",
       "      <td>38</td>\n",
       "      <td>177</td>\n",
       "      <td>5.368252</td>\n",
       "      <td>6.765946</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>7.175573</td>\n",
       "      <td>14.515096</td>\n",
       "      <td>1.901743</td>\n",
       "      <td>9.231263</td>\n",
       "      <td>9.600810</td>\n",
       "      <td>10.058844</td>\n",
       "      <td>5.680021</td>\n",
       "      <td>8.238473</td>\n",
       "      <td>0.272760</td>\n",
       "      <td>11.892743</td>\n",
       "      <td>4.030567</td>\n",
       "      <td>14.946749</td>\n",
       "      <td>7.121632</td>\n",
       "      <td>6.507572</td>\n",
       "      <td>12.462688</td>\n",
       "      <td>12.222522</td>\n",
       "      <td>0.318528</td>\n",
       "      <td>5.350321</td>\n",
       "      <td>3.143358</td>\n",
       "      <td>10.291804</td>\n",
       "      <td>13.105170</td>\n",
       "      <td>10.159100</td>\n",
       "      <td>3.671488</td>\n",
       "      <td>11.087198</td>\n",
       "      <td>1.289054</td>\n",
       "      <td>0.249375</td>\n",
       "      <td>2.967133</td>\n",
       "      <td>6.885179</td>\n",
       "      <td>2.789577</td>\n",
       "      <td>6.416708</td>\n",
       "      <td>10.549814</td>\n",
       "      <td>11.456437</td>\n",
       "      <td>6.468099</td>\n",
       "      <td>2.519049</td>\n",
       "      <td>0.258284</td>\n",
       "      <td>9.317696</td>\n",
       "      <td>5.383098</td>\n",
       "      <td>69.424645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>359</td>\n",
       "      <td>20</td>\n",
       "      <td>218</td>\n",
       "      <td>317</td>\n",
       "      <td>301</td>\n",
       "      <td>8.124038</td>\n",
       "      <td>0.767304</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>141</td>\n",
       "      <td>417</td>\n",
       "      <td>343</td>\n",
       "      <td>14.592218</td>\n",
       "      <td>6.483629</td>\n",
       "      <td>9.159313</td>\n",
       "      <td>5.083046</td>\n",
       "      <td>8.772015</td>\n",
       "      <td>7.687242</td>\n",
       "      <td>11.106926</td>\n",
       "      <td>13.926358</td>\n",
       "      <td>0.914295</td>\n",
       "      <td>1.418124</td>\n",
       "      <td>4.504223</td>\n",
       "      <td>6.158475</td>\n",
       "      <td>1.790923</td>\n",
       "      <td>7.049614</td>\n",
       "      <td>14.409808</td>\n",
       "      <td>11.615837</td>\n",
       "      <td>5.675790</td>\n",
       "      <td>9.136146</td>\n",
       "      <td>10.640432</td>\n",
       "      <td>14.051122</td>\n",
       "      <td>2.240417</td>\n",
       "      <td>3.271828</td>\n",
       "      <td>5.061455</td>\n",
       "      <td>3.679880</td>\n",
       "      <td>2.948615</td>\n",
       "      <td>6.419577</td>\n",
       "      <td>14.873100</td>\n",
       "      <td>14.806887</td>\n",
       "      <td>9.992545</td>\n",
       "      <td>3.701959</td>\n",
       "      <td>11.162686</td>\n",
       "      <td>1.886560</td>\n",
       "      <td>1.919999</td>\n",
       "      <td>2.268203</td>\n",
       "      <td>0.149421</td>\n",
       "      <td>4.105907</td>\n",
       "      <td>10.416291</td>\n",
       "      <td>6.816217</td>\n",
       "      <td>8.586960</td>\n",
       "      <td>4.512419</td>\n",
       "      <td>66.767304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7  8    9   10   11   12  \\\n",
       "0  236  488   16  221  382   97 -4.472136  0.107472  0  132  281  336   99   \n",
       "1  386  206  357  232    1  198  7.810250  0.763713  1  143  109  123  130   \n",
       "2  429   49  481  111  111  146  8.602325  0.651162  1  430  488  138   80   \n",
       "3  414  350  481  370  208  158  8.306624  0.424645  1  340  111   38  177   \n",
       "4  318  359   20  218  317  301  8.124038  0.767304  1  212  141  417  343   \n",
       "\n",
       "          13         14         15         16         17         18  \\\n",
       "0   3.880098   1.797502   3.252475  12.131981   3.091361   5.630319   \n",
       "1  12.099770  10.670550  14.137111   0.217037   1.426881   0.916617   \n",
       "2   2.947479  12.671352  13.279918  13.163338   9.051826  11.605822   \n",
       "3   5.368252   6.765946   0.544415   7.175573  14.515096   1.901743   \n",
       "4  14.592218   6.483629   9.159313   5.083046   8.772015   7.687242   \n",
       "\n",
       "          19         20         21         22         23         24  \\\n",
       "0   4.466373   2.511203   5.982724   4.541159  12.740476  12.634929   \n",
       "1   8.168254   0.432319   5.872218  10.401401  12.843301   7.529992   \n",
       "2  11.094161  10.461813   9.827713   2.206019   9.914789   4.448482   \n",
       "3   9.231263   9.600810  10.058844   5.680021   8.238473   0.272760   \n",
       "4  11.106926  13.926358   0.914295   1.418124   4.504223   6.158475   \n",
       "\n",
       "          25         26         27         28         29         30  \\\n",
       "0   4.050294  11.827245   3.568321  13.420537   8.251807   2.287900   \n",
       "1  12.525335   8.655344   8.499587   5.602552   4.187738  13.205982   \n",
       "2   8.864810  10.837476  14.167872  13.456857  14.855511   7.596095   \n",
       "3  11.892743   4.030567  14.946749   7.121632   6.507572  12.462688   \n",
       "4   1.790923   7.049614  14.409808  11.615837   5.675790   9.136146   \n",
       "\n",
       "          31         32         33        34         35         36         37  \\\n",
       "0  14.834430   0.082253   2.975561  5.223753   1.212287   7.302797   7.083149   \n",
       "1   5.432667  12.379275  11.938420  6.057282   2.581280  11.785456   4.445564   \n",
       "2   4.928033   2.439930   6.791165  8.709536   1.363087   4.980975   2.182372   \n",
       "3  12.222522   0.318528   5.350321  3.143358  10.291804  13.105170  10.159100   \n",
       "4  10.640432  14.051122   2.240417  3.271828   5.061455   3.679880   2.948615   \n",
       "\n",
       "          38         39         40        41         42         43         44  \\\n",
       "0   3.610350   7.767512   7.829657  8.395356   1.583711  10.125020  13.340874   \n",
       "1  11.141346   0.839731   1.481908  8.920653   4.450379  10.584802  12.484882   \n",
       "2  14.673614   8.083289   6.601472  6.789200  12.982035  10.273114  14.030257   \n",
       "3   3.671488  11.087198   1.289054  0.249375   2.967133   6.885179   2.789577   \n",
       "4   6.419577  14.873100  14.806887  9.992545   3.701959  11.162686   1.886560   \n",
       "\n",
       "         45         46         47         48         49         50         51  \\\n",
       "0  0.870542   1.962937   7.466666  11.547794   8.822916   9.046424   7.895535   \n",
       "1  7.168680   2.885415  12.413973  10.260494  10.091351   9.270888   3.173994   \n",
       "2  0.394970   8.160625  12.592059   8.937577   2.265191  11.255721  12.794841   \n",
       "3  6.416708  10.549814  11.456437   6.468099   2.519049   0.258284   9.317696   \n",
       "4  1.919999   2.268203   0.149421   4.105907  10.416291   6.816217   8.586960   \n",
       "\n",
       "          52     target  \n",
       "0  11.010677  20.107472  \n",
       "1  13.921871  61.763713  \n",
       "2  12.080951  74.651162  \n",
       "3   5.383098  69.424645  \n",
       "4   4.512419  66.767304  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look into dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want change initial dataframe, \n",
    "# so make copy of that\n",
    "# and delete feature 6 from it\n",
    "dataset_exp = dataset.copy()\n",
    "dataset_exp.drop(columns=['6'], inplace=True)\n",
    "dataset_exp_columns = list(dataset_exp.columns)\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation all columns ok. We scale target value also.\n",
    "scaling_feature = dataset_exp_columns\n",
    "dataset_scaling = scaler.fit_transform(dataset_exp[scaling_feature])\n",
    "dataset_scaling = pd.DataFrame(dataset_scaling)\n",
    "dataset_scaling.columns = dataset_exp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472946</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.442886</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.194389</td>\n",
       "      <td>0.107460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264529</td>\n",
       "      <td>0.563126</td>\n",
       "      <td>0.673347</td>\n",
       "      <td>0.198397</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.216827</td>\n",
       "      <td>0.808797</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.375349</td>\n",
       "      <td>0.297760</td>\n",
       "      <td>0.167393</td>\n",
       "      <td>0.398846</td>\n",
       "      <td>0.302742</td>\n",
       "      <td>0.849368</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.270017</td>\n",
       "      <td>0.788483</td>\n",
       "      <td>0.237887</td>\n",
       "      <td>0.894704</td>\n",
       "      <td>0.550113</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.198367</td>\n",
       "      <td>0.348258</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.472212</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.105564</td>\n",
       "      <td>0.675018</td>\n",
       "      <td>0.889397</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>0.497792</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.588197</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.526387</td>\n",
       "      <td>0.734054</td>\n",
       "      <td>0.201055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773547</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>0.464930</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.218437</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.260521</td>\n",
       "      <td>0.806654</td>\n",
       "      <td>0.711368</td>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.391479</td>\n",
       "      <td>0.693435</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.577023</td>\n",
       "      <td>0.566643</td>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.279165</td>\n",
       "      <td>0.880410</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.825290</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.403828</td>\n",
       "      <td>0.172082</td>\n",
       "      <td>0.785697</td>\n",
       "      <td>0.296370</td>\n",
       "      <td>0.742758</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>0.098789</td>\n",
       "      <td>0.594705</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.832331</td>\n",
       "      <td>0.477926</td>\n",
       "      <td>0.192356</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.684031</td>\n",
       "      <td>0.672760</td>\n",
       "      <td>0.618057</td>\n",
       "      <td>0.211601</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.617630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.292585</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>0.160321</td>\n",
       "      <td>0.196495</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.885346</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>0.773725</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.697449</td>\n",
       "      <td>0.655194</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>0.660983</td>\n",
       "      <td>0.296560</td>\n",
       "      <td>0.590996</td>\n",
       "      <td>0.722498</td>\n",
       "      <td>0.944535</td>\n",
       "      <td>0.897125</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.506411</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>0.162659</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>0.580651</td>\n",
       "      <td>0.090868</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>0.538885</td>\n",
       "      <td>0.440103</td>\n",
       "      <td>0.452605</td>\n",
       "      <td>0.865481</td>\n",
       "      <td>0.684891</td>\n",
       "      <td>0.935357</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.839499</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.151013</td>\n",
       "      <td>0.750381</td>\n",
       "      <td>0.853024</td>\n",
       "      <td>0.805407</td>\n",
       "      <td>0.746509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.741483</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.316633</td>\n",
       "      <td>0.424643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681363</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.076152</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.357881</td>\n",
       "      <td>0.451056</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>0.478366</td>\n",
       "      <td>0.967683</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>0.615429</td>\n",
       "      <td>0.640047</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.378668</td>\n",
       "      <td>0.549225</td>\n",
       "      <td>0.018176</td>\n",
       "      <td>0.792865</td>\n",
       "      <td>0.268704</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.433826</td>\n",
       "      <td>0.830856</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.356686</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.686123</td>\n",
       "      <td>0.873678</td>\n",
       "      <td>0.677279</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.739148</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.427792</td>\n",
       "      <td>0.703330</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.431196</td>\n",
       "      <td>0.167937</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.621202</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.694242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637275</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.635271</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.767310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.687375</td>\n",
       "      <td>0.972820</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.610630</td>\n",
       "      <td>0.338862</td>\n",
       "      <td>0.584805</td>\n",
       "      <td>0.512480</td>\n",
       "      <td>0.740478</td>\n",
       "      <td>0.928425</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.094535</td>\n",
       "      <td>0.300266</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>0.119386</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.960664</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.378372</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.936748</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.218125</td>\n",
       "      <td>0.337429</td>\n",
       "      <td>0.245324</td>\n",
       "      <td>0.196572</td>\n",
       "      <td>0.427972</td>\n",
       "      <td>0.991544</td>\n",
       "      <td>0.987145</td>\n",
       "      <td>0.666166</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.744198</td>\n",
       "      <td>0.125770</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.694422</td>\n",
       "      <td>0.454410</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>0.667668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         7    8  \\\n",
       "0  0.472946  0.977956  0.032064  0.442886  0.765531  0.194389  0.107460  0.0   \n",
       "1  0.773547  0.412826  0.715431  0.464930  0.002004  0.396794  0.763720  1.0   \n",
       "2  0.859719  0.098196  0.963928  0.222445  0.222445  0.292585  0.651165  1.0   \n",
       "3  0.829659  0.701403  0.963928  0.741483  0.416834  0.316633  0.424643  1.0   \n",
       "4  0.637275  0.719439  0.040080  0.436874  0.635271  0.603206  0.767310  1.0   \n",
       "\n",
       "          9        10        11        12        13        14        15  \\\n",
       "0  0.264529  0.563126  0.673347  0.198397  0.258670  0.119819  0.216827   \n",
       "1  0.286573  0.218437  0.246493  0.260521  0.806654  0.711368  0.942495   \n",
       "2  0.861723  0.977956  0.276553  0.160321  0.196495  0.844758  0.885346   \n",
       "3  0.681363  0.222445  0.076152  0.354709  0.357881  0.451056  0.036283   \n",
       "4  0.424850  0.282565  0.835671  0.687375  0.972820  0.432234  0.610630   \n",
       "\n",
       "         16        17        18        19        20        21        22  \\\n",
       "0  0.808797  0.206089  0.375349  0.297760  0.167393  0.398846  0.302742   \n",
       "1  0.014457  0.095122  0.061094  0.544560  0.028797  0.391479  0.693435   \n",
       "2  0.877556  0.603460  0.773725  0.739627  0.697449  0.655194  0.147062   \n",
       "3  0.478366  0.967683  0.126771  0.615429  0.640047  0.670604  0.378668   \n",
       "4  0.338862  0.584805  0.512480  0.740478  0.928425  0.060930  0.094535   \n",
       "\n",
       "         23        24        25        26        27        28        29  \\\n",
       "0  0.849368  0.842328  0.270017  0.788483  0.237887  0.894704  0.550113   \n",
       "1  0.856224  0.501996  0.835040  0.577023  0.566643  0.373498  0.279165   \n",
       "2  0.660983  0.296560  0.590996  0.722498  0.944535  0.897125  0.990378   \n",
       "3  0.549225  0.018176  0.792865  0.268704  0.996461  0.474771  0.433826   \n",
       "4  0.300266  0.410560  0.119386  0.469974  0.960664  0.774389  0.378372   \n",
       "\n",
       "         30        31        32        33        34        35        36  \\\n",
       "0  0.152526  0.988967  0.005479  0.198367  0.348258  0.080815  0.486853   \n",
       "1  0.880410  0.362177  0.825290  0.795900  0.403828  0.172082  0.785697   \n",
       "2  0.506411  0.328534  0.162659  0.452744  0.580651  0.090868  0.332064   \n",
       "3  0.830856  0.814838  0.021231  0.356686  0.209560  0.686123  0.873678   \n",
       "4  0.609083  0.709364  0.936748  0.149356  0.218125  0.337429  0.245324   \n",
       "\n",
       "         37        38        39        40        41        42        43  \\\n",
       "0  0.472212  0.240690  0.517833  0.521984  0.559685  0.105564  0.675018   \n",
       "1  0.296370  0.742758  0.055976  0.098789  0.594705  0.296682  0.705671   \n",
       "2  0.145488  0.978244  0.538885  0.440103  0.452605  0.865481  0.684891   \n",
       "3  0.677279  0.244766  0.739148  0.085932  0.016607  0.197795  0.459022   \n",
       "4  0.196572  0.427972  0.991544  0.987145  0.666166  0.246786  0.744198   \n",
       "\n",
       "         44        45        46        47        48        49        50  \\\n",
       "0  0.889397  0.058035  0.130856  0.497792  0.769854  0.588197  0.603093   \n",
       "1  0.832331  0.477926  0.192356  0.827626  0.684031  0.672760  0.618057   \n",
       "2  0.935357  0.026330  0.544047  0.839499  0.595834  0.151013  0.750381   \n",
       "3  0.185972  0.427792  0.703330  0.763788  0.431196  0.167937  0.017208   \n",
       "4  0.125770  0.128002  0.151208  0.009956  0.273712  0.694422  0.454410   \n",
       "\n",
       "         51        52    target  \n",
       "0  0.526387  0.734054  0.201055  \n",
       "1  0.211601  0.928138  0.617630  \n",
       "2  0.853024  0.805407  0.746509  \n",
       "3  0.621202  0.358873  0.694242  \n",
       "4  0.572484  0.300827  0.667668  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scaling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Modeling DEL 6: Sklearn LinearRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data on X and y\n",
    "X_data = dataset_scaling.loc[:, dataset_scaling.columns!='target']\n",
    "y_data = dataset_scaling['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.28907\n",
      "RMSE scores: [0.29004 0.2882  0.29007 0.28758 0.28948]\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['LinearRegression_DEL_6'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]  \n",
       "Mean_RMSE                                              0.28907  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Modeling DEL 6: Sklearn MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.29029\n",
      "RMSE scores: [0.29245 0.29017 0.29039 0.28825 0.29018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use MLPClassifier\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=10, random_state=1)\n",
    "rmse_scores = cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error')\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['MLPRegressor_50_relu_DEL_6'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "      <td>0.29029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28907   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_6  \n",
       "RMSE_Cross_Val_5  [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]  \n",
       "Mean_RMSE                                               0.29029  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that after deleting feature 6, Linear Regression score doesn't change, but MLPRegression has worse result(the same as usual LinearRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Delete feature 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want change initial dataframe, \n",
    "# so make copy of that\n",
    "# and delete feature 6 from it\n",
    "dataset_exp = dataset.copy()\n",
    "dataset_exp.drop(columns=['8'], inplace=True)\n",
    "dataset_exp_columns = list(dataset_exp.columns)\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation all columns ok. We scale target value also.\n",
    "scaling_feature = dataset_exp_columns\n",
    "dataset_scaling = scaler.fit_transform(dataset_exp[scaling_feature])\n",
    "dataset_scaling = pd.DataFrame(dataset_scaling)\n",
    "dataset_scaling.columns = dataset_exp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472946</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.442886</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.194389</td>\n",
       "      <td>0.275267</td>\n",
       "      <td>0.107460</td>\n",
       "      <td>0.264529</td>\n",
       "      <td>0.563126</td>\n",
       "      <td>0.673347</td>\n",
       "      <td>0.198397</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.216827</td>\n",
       "      <td>0.808797</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.375349</td>\n",
       "      <td>0.297760</td>\n",
       "      <td>0.167393</td>\n",
       "      <td>0.398846</td>\n",
       "      <td>0.302742</td>\n",
       "      <td>0.849368</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.270017</td>\n",
       "      <td>0.788483</td>\n",
       "      <td>0.237887</td>\n",
       "      <td>0.894704</td>\n",
       "      <td>0.550113</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.198367</td>\n",
       "      <td>0.348258</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.472212</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.105564</td>\n",
       "      <td>0.675018</td>\n",
       "      <td>0.889397</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>0.497792</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.588197</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.526387</td>\n",
       "      <td>0.734054</td>\n",
       "      <td>0.201055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773547</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>0.464930</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.892480</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.218437</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.260521</td>\n",
       "      <td>0.806654</td>\n",
       "      <td>0.711368</td>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.391479</td>\n",
       "      <td>0.693435</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.577023</td>\n",
       "      <td>0.566643</td>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.279165</td>\n",
       "      <td>0.880410</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.825290</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.403828</td>\n",
       "      <td>0.172082</td>\n",
       "      <td>0.785697</td>\n",
       "      <td>0.296370</td>\n",
       "      <td>0.742758</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>0.098789</td>\n",
       "      <td>0.594705</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.832331</td>\n",
       "      <td>0.477926</td>\n",
       "      <td>0.192356</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.684031</td>\n",
       "      <td>0.672760</td>\n",
       "      <td>0.618057</td>\n",
       "      <td>0.211601</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.617630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.292585</td>\n",
       "      <td>0.932283</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>0.160321</td>\n",
       "      <td>0.196495</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.885346</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>0.773725</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.697449</td>\n",
       "      <td>0.655194</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>0.660983</td>\n",
       "      <td>0.296560</td>\n",
       "      <td>0.590996</td>\n",
       "      <td>0.722498</td>\n",
       "      <td>0.944535</td>\n",
       "      <td>0.897125</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.506411</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>0.162659</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>0.580651</td>\n",
       "      <td>0.090868</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>0.538885</td>\n",
       "      <td>0.440103</td>\n",
       "      <td>0.452605</td>\n",
       "      <td>0.865481</td>\n",
       "      <td>0.684891</td>\n",
       "      <td>0.935357</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.839499</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.151013</td>\n",
       "      <td>0.750381</td>\n",
       "      <td>0.853024</td>\n",
       "      <td>0.805407</td>\n",
       "      <td>0.746509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.741483</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.316633</td>\n",
       "      <td>0.917424</td>\n",
       "      <td>0.424643</td>\n",
       "      <td>0.681363</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.076152</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.357881</td>\n",
       "      <td>0.451056</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>0.478366</td>\n",
       "      <td>0.967683</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>0.615429</td>\n",
       "      <td>0.640047</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.378668</td>\n",
       "      <td>0.549225</td>\n",
       "      <td>0.018176</td>\n",
       "      <td>0.792865</td>\n",
       "      <td>0.268704</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.433826</td>\n",
       "      <td>0.830856</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.356686</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.686123</td>\n",
       "      <td>0.873678</td>\n",
       "      <td>0.677279</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.739148</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.427792</td>\n",
       "      <td>0.703330</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.431196</td>\n",
       "      <td>0.167937</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.621202</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.694242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637275</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.635271</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.908248</td>\n",
       "      <td>0.767310</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.687375</td>\n",
       "      <td>0.972820</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.610630</td>\n",
       "      <td>0.338862</td>\n",
       "      <td>0.584805</td>\n",
       "      <td>0.512480</td>\n",
       "      <td>0.740478</td>\n",
       "      <td>0.928425</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.094535</td>\n",
       "      <td>0.300266</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>0.119386</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.960664</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.378372</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.936748</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.218125</td>\n",
       "      <td>0.337429</td>\n",
       "      <td>0.245324</td>\n",
       "      <td>0.196572</td>\n",
       "      <td>0.427972</td>\n",
       "      <td>0.991544</td>\n",
       "      <td>0.987145</td>\n",
       "      <td>0.666166</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.744198</td>\n",
       "      <td>0.125770</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.694422</td>\n",
       "      <td>0.454410</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>0.667668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.472946  0.977956  0.032064  0.442886  0.765531  0.194389  0.275267   \n",
       "1  0.773547  0.412826  0.715431  0.464930  0.002004  0.396794  0.892480   \n",
       "2  0.859719  0.098196  0.963928  0.222445  0.222445  0.292585  0.932283   \n",
       "3  0.829659  0.701403  0.963928  0.741483  0.416834  0.316633  0.917424   \n",
       "4  0.637275  0.719439  0.040080  0.436874  0.635271  0.603206  0.908248   \n",
       "\n",
       "          7         9        10        11        12        13        14  \\\n",
       "0  0.107460  0.264529  0.563126  0.673347  0.198397  0.258670  0.119819   \n",
       "1  0.763720  0.286573  0.218437  0.246493  0.260521  0.806654  0.711368   \n",
       "2  0.651165  0.861723  0.977956  0.276553  0.160321  0.196495  0.844758   \n",
       "3  0.424643  0.681363  0.222445  0.076152  0.354709  0.357881  0.451056   \n",
       "4  0.767310  0.424850  0.282565  0.835671  0.687375  0.972820  0.432234   \n",
       "\n",
       "         15        16        17        18        19        20        21  \\\n",
       "0  0.216827  0.808797  0.206089  0.375349  0.297760  0.167393  0.398846   \n",
       "1  0.942495  0.014457  0.095122  0.061094  0.544560  0.028797  0.391479   \n",
       "2  0.885346  0.877556  0.603460  0.773725  0.739627  0.697449  0.655194   \n",
       "3  0.036283  0.478366  0.967683  0.126771  0.615429  0.640047  0.670604   \n",
       "4  0.610630  0.338862  0.584805  0.512480  0.740478  0.928425  0.060930   \n",
       "\n",
       "         22        23        24        25        26        27        28  \\\n",
       "0  0.302742  0.849368  0.842328  0.270017  0.788483  0.237887  0.894704   \n",
       "1  0.693435  0.856224  0.501996  0.835040  0.577023  0.566643  0.373498   \n",
       "2  0.147062  0.660983  0.296560  0.590996  0.722498  0.944535  0.897125   \n",
       "3  0.378668  0.549225  0.018176  0.792865  0.268704  0.996461  0.474771   \n",
       "4  0.094535  0.300266  0.410560  0.119386  0.469974  0.960664  0.774389   \n",
       "\n",
       "         29        30        31        32        33        34        35  \\\n",
       "0  0.550113  0.152526  0.988967  0.005479  0.198367  0.348258  0.080815   \n",
       "1  0.279165  0.880410  0.362177  0.825290  0.795900  0.403828  0.172082   \n",
       "2  0.990378  0.506411  0.328534  0.162659  0.452744  0.580651  0.090868   \n",
       "3  0.433826  0.830856  0.814838  0.021231  0.356686  0.209560  0.686123   \n",
       "4  0.378372  0.609083  0.709364  0.936748  0.149356  0.218125  0.337429   \n",
       "\n",
       "         36        37        38        39        40        41        42  \\\n",
       "0  0.486853  0.472212  0.240690  0.517833  0.521984  0.559685  0.105564   \n",
       "1  0.785697  0.296370  0.742758  0.055976  0.098789  0.594705  0.296682   \n",
       "2  0.332064  0.145488  0.978244  0.538885  0.440103  0.452605  0.865481   \n",
       "3  0.873678  0.677279  0.244766  0.739148  0.085932  0.016607  0.197795   \n",
       "4  0.245324  0.196572  0.427972  0.991544  0.987145  0.666166  0.246786   \n",
       "\n",
       "         43        44        45        46        47        48        49  \\\n",
       "0  0.675018  0.889397  0.058035  0.130856  0.497792  0.769854  0.588197   \n",
       "1  0.705671  0.832331  0.477926  0.192356  0.827626  0.684031  0.672760   \n",
       "2  0.684891  0.935357  0.026330  0.544047  0.839499  0.595834  0.151013   \n",
       "3  0.459022  0.185972  0.427792  0.703330  0.763788  0.431196  0.167937   \n",
       "4  0.744198  0.125770  0.128002  0.151208  0.009956  0.273712  0.694422   \n",
       "\n",
       "         50        51        52    target  \n",
       "0  0.603093  0.526387  0.734054  0.201055  \n",
       "1  0.618057  0.211601  0.928138  0.617630  \n",
       "2  0.750381  0.853024  0.805407  0.746509  \n",
       "3  0.017208  0.621202  0.358873  0.694242  \n",
       "4  0.454410  0.572484  0.300827  0.667668  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scaling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Modeling DEL 8: Sklearn LinearRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data on X and y\n",
    "X_data = dataset_scaling.loc[:, dataset_scaling.columns!='target']\n",
    "y_data = dataset_scaling['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.28908\n",
      "RMSE scores: [0.29004 0.2882  0.29008 0.28758 0.28948]\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['LinearRegression_DEL_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "      <th>LinearRegression_DEL_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29008, 0.28758, 0.28948]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "      <td>0.29029</td>\n",
       "      <td>0.28908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28907   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]   \n",
       "Mean_RMSE                                               0.29029   \n",
       "\n",
       "                                        LinearRegression_DEL_8  \n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29008, 0.28758, 0.28948]  \n",
       "Mean_RMSE                                              0.28908  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Modeling DEL 8: Sklearn MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.02376\n",
      "RMSE scores: [0.02501 0.02266 0.02254 0.02417 0.02441]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use MLPClassifier\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=10, random_state=1)\n",
    "rmse_scores = cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error')\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['MLPRegressor_50_relu_DEL_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "      <th>LinearRegression_DEL_8</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29008, 0.28758, 0.28948]</td>\n",
       "      <td>[0.02501, 0.02266, 0.02254, 0.02417, 0.02441]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "      <td>0.29029</td>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28907   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]   \n",
       "Mean_RMSE                                               0.29029   \n",
       "\n",
       "                                        LinearRegression_DEL_8  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29008, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_8  \n",
       "RMSE_Cross_Val_5  [0.02501, 0.02266, 0.02254, 0.02417, 0.02441]  \n",
       "Mean_RMSE                                               0.02376  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deleting feature 8, MLPRegression return it better RMSE. So for this model, better without feature 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Multiply feature 8 and feature 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This influence in feature 6, in way, become 0, but that have 1's, will not change. So it improve influence of features that have values not close to 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want change initial dataframe, \n",
    "# so make copy of that\n",
    "# and delete feature 6 from it\n",
    "dataset_exp = dataset.copy()\n",
    "dataset_exp['6_8'] = dataset_exp['6']*dataset_exp['8']\n",
    "dataset_exp.drop(columns=['8', '6'], inplace=True)\n",
    "dataset_exp_columns = list(dataset_exp.columns)\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation all columns ok. We scale target value also.\n",
    "scaling_feature = dataset_exp_columns\n",
    "dataset_scaling = scaler.fit_transform(dataset_exp[scaling_feature])\n",
    "dataset_scaling = pd.DataFrame(dataset_scaling)\n",
    "dataset_scaling.columns = dataset_exp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "      <th>6_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472946</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.442886</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.194389</td>\n",
       "      <td>0.107460</td>\n",
       "      <td>0.264529</td>\n",
       "      <td>0.563126</td>\n",
       "      <td>0.673347</td>\n",
       "      <td>0.198397</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.216827</td>\n",
       "      <td>0.808797</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.375349</td>\n",
       "      <td>0.297760</td>\n",
       "      <td>0.167393</td>\n",
       "      <td>0.398846</td>\n",
       "      <td>0.302742</td>\n",
       "      <td>0.849368</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.270017</td>\n",
       "      <td>0.788483</td>\n",
       "      <td>0.237887</td>\n",
       "      <td>0.894704</td>\n",
       "      <td>0.550113</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.198367</td>\n",
       "      <td>0.348258</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.472212</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.105564</td>\n",
       "      <td>0.675018</td>\n",
       "      <td>0.889397</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>0.497792</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.588197</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.526387</td>\n",
       "      <td>0.734054</td>\n",
       "      <td>0.201055</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773547</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>0.464930</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.218437</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.260521</td>\n",
       "      <td>0.806654</td>\n",
       "      <td>0.711368</td>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.391479</td>\n",
       "      <td>0.693435</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.577023</td>\n",
       "      <td>0.566643</td>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.279165</td>\n",
       "      <td>0.880410</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.825290</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.403828</td>\n",
       "      <td>0.172082</td>\n",
       "      <td>0.785697</td>\n",
       "      <td>0.296370</td>\n",
       "      <td>0.742758</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>0.098789</td>\n",
       "      <td>0.594705</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.832331</td>\n",
       "      <td>0.477926</td>\n",
       "      <td>0.192356</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.684031</td>\n",
       "      <td>0.672760</td>\n",
       "      <td>0.618057</td>\n",
       "      <td>0.211601</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.617630</td>\n",
       "      <td>0.784960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.292585</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>0.160321</td>\n",
       "      <td>0.196495</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.885346</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>0.773725</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.697449</td>\n",
       "      <td>0.655194</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>0.660983</td>\n",
       "      <td>0.296560</td>\n",
       "      <td>0.590996</td>\n",
       "      <td>0.722498</td>\n",
       "      <td>0.944535</td>\n",
       "      <td>0.897125</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.506411</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>0.162659</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>0.580651</td>\n",
       "      <td>0.090868</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>0.538885</td>\n",
       "      <td>0.440103</td>\n",
       "      <td>0.452605</td>\n",
       "      <td>0.865481</td>\n",
       "      <td>0.684891</td>\n",
       "      <td>0.935357</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.839499</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.151013</td>\n",
       "      <td>0.750381</td>\n",
       "      <td>0.853024</td>\n",
       "      <td>0.805407</td>\n",
       "      <td>0.746509</td>\n",
       "      <td>0.864566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.741483</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.316633</td>\n",
       "      <td>0.424643</td>\n",
       "      <td>0.681363</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.076152</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.357881</td>\n",
       "      <td>0.451056</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>0.478366</td>\n",
       "      <td>0.967683</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>0.615429</td>\n",
       "      <td>0.640047</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.378668</td>\n",
       "      <td>0.549225</td>\n",
       "      <td>0.018176</td>\n",
       "      <td>0.792865</td>\n",
       "      <td>0.268704</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.433826</td>\n",
       "      <td>0.830856</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.356686</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.686123</td>\n",
       "      <td>0.873678</td>\n",
       "      <td>0.677279</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.739148</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.427792</td>\n",
       "      <td>0.703330</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.431196</td>\n",
       "      <td>0.167937</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.621202</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.694242</td>\n",
       "      <td>0.834847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637275</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.635271</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.767310</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.687375</td>\n",
       "      <td>0.972820</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.610630</td>\n",
       "      <td>0.338862</td>\n",
       "      <td>0.584805</td>\n",
       "      <td>0.512480</td>\n",
       "      <td>0.740478</td>\n",
       "      <td>0.928425</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.094535</td>\n",
       "      <td>0.300266</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>0.119386</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.960664</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.378372</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.936748</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.218125</td>\n",
       "      <td>0.337429</td>\n",
       "      <td>0.245324</td>\n",
       "      <td>0.196572</td>\n",
       "      <td>0.427972</td>\n",
       "      <td>0.991544</td>\n",
       "      <td>0.987145</td>\n",
       "      <td>0.666166</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.744198</td>\n",
       "      <td>0.125770</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.694422</td>\n",
       "      <td>0.454410</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>0.667668</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         7  \\\n",
       "0  0.472946  0.977956  0.032064  0.442886  0.765531  0.194389  0.107460   \n",
       "1  0.773547  0.412826  0.715431  0.464930  0.002004  0.396794  0.763720   \n",
       "2  0.859719  0.098196  0.963928  0.222445  0.222445  0.292585  0.651165   \n",
       "3  0.829659  0.701403  0.963928  0.741483  0.416834  0.316633  0.424643   \n",
       "4  0.637275  0.719439  0.040080  0.436874  0.635271  0.603206  0.767310   \n",
       "\n",
       "          9        10        11        12        13        14        15  \\\n",
       "0  0.264529  0.563126  0.673347  0.198397  0.258670  0.119819  0.216827   \n",
       "1  0.286573  0.218437  0.246493  0.260521  0.806654  0.711368  0.942495   \n",
       "2  0.861723  0.977956  0.276553  0.160321  0.196495  0.844758  0.885346   \n",
       "3  0.681363  0.222445  0.076152  0.354709  0.357881  0.451056  0.036283   \n",
       "4  0.424850  0.282565  0.835671  0.687375  0.972820  0.432234  0.610630   \n",
       "\n",
       "         16        17        18        19        20        21        22  \\\n",
       "0  0.808797  0.206089  0.375349  0.297760  0.167393  0.398846  0.302742   \n",
       "1  0.014457  0.095122  0.061094  0.544560  0.028797  0.391479  0.693435   \n",
       "2  0.877556  0.603460  0.773725  0.739627  0.697449  0.655194  0.147062   \n",
       "3  0.478366  0.967683  0.126771  0.615429  0.640047  0.670604  0.378668   \n",
       "4  0.338862  0.584805  0.512480  0.740478  0.928425  0.060930  0.094535   \n",
       "\n",
       "         23        24        25        26        27        28        29  \\\n",
       "0  0.849368  0.842328  0.270017  0.788483  0.237887  0.894704  0.550113   \n",
       "1  0.856224  0.501996  0.835040  0.577023  0.566643  0.373498  0.279165   \n",
       "2  0.660983  0.296560  0.590996  0.722498  0.944535  0.897125  0.990378   \n",
       "3  0.549225  0.018176  0.792865  0.268704  0.996461  0.474771  0.433826   \n",
       "4  0.300266  0.410560  0.119386  0.469974  0.960664  0.774389  0.378372   \n",
       "\n",
       "         30        31        32        33        34        35        36  \\\n",
       "0  0.152526  0.988967  0.005479  0.198367  0.348258  0.080815  0.486853   \n",
       "1  0.880410  0.362177  0.825290  0.795900  0.403828  0.172082  0.785697   \n",
       "2  0.506411  0.328534  0.162659  0.452744  0.580651  0.090868  0.332064   \n",
       "3  0.830856  0.814838  0.021231  0.356686  0.209560  0.686123  0.873678   \n",
       "4  0.609083  0.709364  0.936748  0.149356  0.218125  0.337429  0.245324   \n",
       "\n",
       "         37        38        39        40        41        42        43  \\\n",
       "0  0.472212  0.240690  0.517833  0.521984  0.559685  0.105564  0.675018   \n",
       "1  0.296370  0.742758  0.055976  0.098789  0.594705  0.296682  0.705671   \n",
       "2  0.145488  0.978244  0.538885  0.440103  0.452605  0.865481  0.684891   \n",
       "3  0.677279  0.244766  0.739148  0.085932  0.016607  0.197795  0.459022   \n",
       "4  0.196572  0.427972  0.991544  0.987145  0.666166  0.246786  0.744198   \n",
       "\n",
       "         44        45        46        47        48        49        50  \\\n",
       "0  0.889397  0.058035  0.130856  0.497792  0.769854  0.588197  0.603093   \n",
       "1  0.832331  0.477926  0.192356  0.827626  0.684031  0.672760  0.618057   \n",
       "2  0.935357  0.026330  0.544047  0.839499  0.595834  0.151013  0.750381   \n",
       "3  0.185972  0.427792  0.703330  0.763788  0.431196  0.167937  0.017208   \n",
       "4  0.125770  0.128002  0.151208  0.009956  0.273712  0.694422  0.454410   \n",
       "\n",
       "         51        52    target       6_8  \n",
       "0  0.526387  0.734054  0.201055  0.000000  \n",
       "1  0.211601  0.928138  0.617630  0.784960  \n",
       "2  0.853024  0.805407  0.746509  0.864566  \n",
       "3  0.621202  0.358873  0.694242  0.834847  \n",
       "4  0.572484  0.300827  0.667668  0.816497  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scaling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Modeling Multi 6_8: Sklearn LinearRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data on X and y\n",
    "X_data = dataset_scaling.loc[:, dataset_scaling.columns!='target']\n",
    "y_data = dataset_scaling['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.27426\n",
      "RMSE scores: [0.27601 0.27295 0.27586 0.27155 0.27493]\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['LinearRegression_Multi6_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Modeling Multi 6_8: Sklearn MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.21206\n",
      "RMSE scores: [0.21263 0.2095  0.2131  0.21362 0.21147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use MLPClassifier\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=10, random_state=1)\n",
    "rmse_scores = cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error')\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['MLPRegressor_50_relu_Multi6_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "      <th>LinearRegression_DEL_8</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_8</th>\n",
       "      <th>LinearRegression_Multi6_8</th>\n",
       "      <th>MLPRegressor_50_relu_Multi6_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29008, 0.28758, 0.28948]</td>\n",
       "      <td>[0.02501, 0.02266, 0.02254, 0.02417, 0.02441]</td>\n",
       "      <td>[0.27601, 0.27295, 0.27586, 0.27155, 0.27493]</td>\n",
       "      <td>[0.21263, 0.2095, 0.2131, 0.21362, 0.21147]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "      <td>0.29029</td>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02376</td>\n",
       "      <td>0.27426</td>\n",
       "      <td>0.21206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28907   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]   \n",
       "Mean_RMSE                                               0.29029   \n",
       "\n",
       "                                        LinearRegression_DEL_8  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29008, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_8  \\\n",
       "RMSE_Cross_Val_5  [0.02501, 0.02266, 0.02254, 0.02417, 0.02441]   \n",
       "Mean_RMSE                                               0.02376   \n",
       "\n",
       "                                      LinearRegression_Multi6_8  \\\n",
       "RMSE_Cross_Val_5  [0.27601, 0.27295, 0.27586, 0.27155, 0.27493]   \n",
       "Mean_RMSE                                               0.27426   \n",
       "\n",
       "                                MLPRegressor_50_relu_Multi6_8  \n",
       "RMSE_Cross_Val_5  [0.21263, 0.2095, 0.2131, 0.21362, 0.21147]  \n",
       "Mean_RMSE                                             0.21206  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.21206\n",
      "RMSE scores: [0.21263 0.2095  0.2131  0.21362 0.21147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use MLPClassifier\n",
    "model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=10, random_state=1)\n",
    "rmse_scores = cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error')\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "result_df['MLPRegressor_50_relu_Multi6_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "      <th>LinearRegression_DEL_8</th>\n",
       "      <th>MLPRegressor_50_relu_DEL_8</th>\n",
       "      <th>LinearRegression_Multi6_8</th>\n",
       "      <th>MLPRegressor_50_relu_Multi6_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "      <td>[0.29004, 0.2882, 0.29008, 0.28758, 0.28948]</td>\n",
       "      <td>[0.02501, 0.02266, 0.02254, 0.02417, 0.02441]</td>\n",
       "      <td>[0.27601, 0.27295, 0.27586, 0.27155, 0.27493]</td>\n",
       "      <td>[0.21263, 0.2095, 0.2131, 0.21362, 0.21147]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02843</td>\n",
       "      <td>0.28907</td>\n",
       "      <td>0.29029</td>\n",
       "      <td>0.28908</td>\n",
       "      <td>0.02376</td>\n",
       "      <td>0.27426</td>\n",
       "      <td>0.21206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LinearRegression  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                           MLPRegressor_50_relu  \\\n",
       "RMSE_Cross_Val_5  [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "Mean_RMSE                                               0.02843   \n",
       "\n",
       "                                        LinearRegression_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28907   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_6  \\\n",
       "RMSE_Cross_Val_5  [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]   \n",
       "Mean_RMSE                                               0.29029   \n",
       "\n",
       "                                        LinearRegression_DEL_8  \\\n",
       "RMSE_Cross_Val_5  [0.29004, 0.2882, 0.29008, 0.28758, 0.28948]   \n",
       "Mean_RMSE                                              0.28908   \n",
       "\n",
       "                                     MLPRegressor_50_relu_DEL_8  \\\n",
       "RMSE_Cross_Val_5  [0.02501, 0.02266, 0.02254, 0.02417, 0.02441]   \n",
       "Mean_RMSE                                               0.02376   \n",
       "\n",
       "                                      LinearRegression_Multi6_8  \\\n",
       "RMSE_Cross_Val_5  [0.27601, 0.27295, 0.27586, 0.27155, 0.27493]   \n",
       "Mean_RMSE                                               0.27426   \n",
       "\n",
       "                                MLPRegressor_50_relu_Multi6_8  \n",
       "RMSE_Cross_Val_5  [0.21263, 0.2095, 0.2131, 0.21362, 0.21147]  \n",
       "Mean_RMSE                                             0.21206  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_inv = result_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE_Cross_Val_5</th>\n",
       "      <th>Mean_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MLPRegressor_50_relu_DEL_8</th>\n",
       "      <td>[0.02501, 0.02266, 0.02254, 0.02417, 0.02441]</td>\n",
       "      <td>0.02376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor_50_relu</th>\n",
       "      <td>[0.03118, 0.02748, 0.03421, 0.02324, 0.02604]</td>\n",
       "      <td>0.02843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor_50_relu_Multi6_8</th>\n",
       "      <td>[0.21263, 0.2095, 0.2131, 0.21362, 0.21147]</td>\n",
       "      <td>0.21206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression_Multi6_8</th>\n",
       "      <td>[0.27601, 0.27295, 0.27586, 0.27155, 0.27493]</td>\n",
       "      <td>0.27426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression_DEL_6</th>\n",
       "      <td>[0.29004, 0.2882, 0.29007, 0.28758, 0.28948]</td>\n",
       "      <td>0.28907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>[0.29004, 0.2882, 0.29009, 0.28758, 0.28949]</td>\n",
       "      <td>0.28908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression_DEL_8</th>\n",
       "      <td>[0.29004, 0.2882, 0.29008, 0.28758, 0.28948]</td>\n",
       "      <td>0.28908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor_50_relu_DEL_6</th>\n",
       "      <td>[0.29245, 0.29017, 0.29039, 0.28825, 0.29018]</td>\n",
       "      <td>0.29029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            RMSE_Cross_Val_5  \\\n",
       "MLPRegressor_50_relu_DEL_8     [0.02501, 0.02266, 0.02254, 0.02417, 0.02441]   \n",
       "MLPRegressor_50_relu           [0.03118, 0.02748, 0.03421, 0.02324, 0.02604]   \n",
       "MLPRegressor_50_relu_Multi6_8    [0.21263, 0.2095, 0.2131, 0.21362, 0.21147]   \n",
       "LinearRegression_Multi6_8      [0.27601, 0.27295, 0.27586, 0.27155, 0.27493]   \n",
       "LinearRegression_DEL_6          [0.29004, 0.2882, 0.29007, 0.28758, 0.28948]   \n",
       "LinearRegression                [0.29004, 0.2882, 0.29009, 0.28758, 0.28949]   \n",
       "LinearRegression_DEL_8          [0.29004, 0.2882, 0.29008, 0.28758, 0.28948]   \n",
       "MLPRegressor_50_relu_DEL_6     [0.29245, 0.29017, 0.29039, 0.28825, 0.29018]   \n",
       "\n",
       "                              Mean_RMSE  \n",
       "MLPRegressor_50_relu_DEL_8      0.02376  \n",
       "MLPRegressor_50_relu            0.02843  \n",
       "MLPRegressor_50_relu_Multi6_8   0.21206  \n",
       "LinearRegression_Multi6_8       0.27426  \n",
       "LinearRegression_DEL_6          0.28907  \n",
       "LinearRegression                0.28908  \n",
       "LinearRegression_DEL_8          0.28908  \n",
       "MLPRegressor_50_relu_DEL_6      0.29029  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_inv.sort_values(by=['Mean_RMSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we have:\n",
    "* The best performing MLPRegressor with `1 hidden layer(50 neurons)` with `activation='relu'` without feature 8. This mpdel the best, but more complex, that makes it similar to \"black box\" - we can't exactly explain logic of it calculation.\n",
    "* The best of simple model - LinearRegression with multiplication feature 6 and 8. Deleting one of these feature have *no influence* on result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Add square features\n",
    "Try to add more complex dependency as squares of initial features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_exp = dataset.copy()\n",
    "dataset_exp_columns = list(dataset_exp.columns)\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy list -  we need square all features without target\n",
    "data_exp_columns_sq = dataset_exp_columns.copy()\n",
    "data_exp_columns_sq.remove('target')\n",
    "data_exp_columns_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square of binary feature to wil be the same binary feature\n",
    "data_exp_columns_sq.remove('8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "      <th>0_square</th>\n",
       "      <th>1_square</th>\n",
       "      <th>2_square</th>\n",
       "      <th>3_square</th>\n",
       "      <th>4_square</th>\n",
       "      <th>5_square</th>\n",
       "      <th>6_square</th>\n",
       "      <th>7_square</th>\n",
       "      <th>9_square</th>\n",
       "      <th>10_square</th>\n",
       "      <th>11_square</th>\n",
       "      <th>12_square</th>\n",
       "      <th>13_square</th>\n",
       "      <th>14_square</th>\n",
       "      <th>15_square</th>\n",
       "      <th>16_square</th>\n",
       "      <th>17_square</th>\n",
       "      <th>18_square</th>\n",
       "      <th>19_square</th>\n",
       "      <th>20_square</th>\n",
       "      <th>21_square</th>\n",
       "      <th>22_square</th>\n",
       "      <th>23_square</th>\n",
       "      <th>24_square</th>\n",
       "      <th>25_square</th>\n",
       "      <th>26_square</th>\n",
       "      <th>27_square</th>\n",
       "      <th>28_square</th>\n",
       "      <th>29_square</th>\n",
       "      <th>30_square</th>\n",
       "      <th>31_square</th>\n",
       "      <th>32_square</th>\n",
       "      <th>33_square</th>\n",
       "      <th>34_square</th>\n",
       "      <th>35_square</th>\n",
       "      <th>36_square</th>\n",
       "      <th>37_square</th>\n",
       "      <th>38_square</th>\n",
       "      <th>39_square</th>\n",
       "      <th>40_square</th>\n",
       "      <th>41_square</th>\n",
       "      <th>42_square</th>\n",
       "      <th>43_square</th>\n",
       "      <th>44_square</th>\n",
       "      <th>45_square</th>\n",
       "      <th>46_square</th>\n",
       "      <th>47_square</th>\n",
       "      <th>48_square</th>\n",
       "      <th>49_square</th>\n",
       "      <th>50_square</th>\n",
       "      <th>51_square</th>\n",
       "      <th>52_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>488</td>\n",
       "      <td>16</td>\n",
       "      <td>221</td>\n",
       "      <td>382</td>\n",
       "      <td>97</td>\n",
       "      <td>-4.472136</td>\n",
       "      <td>0.107472</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>281</td>\n",
       "      <td>336</td>\n",
       "      <td>99</td>\n",
       "      <td>3.880098</td>\n",
       "      <td>1.797502</td>\n",
       "      <td>3.252475</td>\n",
       "      <td>12.131981</td>\n",
       "      <td>3.091361</td>\n",
       "      <td>5.630319</td>\n",
       "      <td>4.466373</td>\n",
       "      <td>2.511203</td>\n",
       "      <td>5.982724</td>\n",
       "      <td>4.541159</td>\n",
       "      <td>12.740476</td>\n",
       "      <td>12.634929</td>\n",
       "      <td>4.050294</td>\n",
       "      <td>11.827245</td>\n",
       "      <td>3.568321</td>\n",
       "      <td>13.420537</td>\n",
       "      <td>8.251807</td>\n",
       "      <td>2.287900</td>\n",
       "      <td>14.834430</td>\n",
       "      <td>0.082253</td>\n",
       "      <td>2.975561</td>\n",
       "      <td>5.223753</td>\n",
       "      <td>1.212287</td>\n",
       "      <td>7.302797</td>\n",
       "      <td>7.083149</td>\n",
       "      <td>3.610350</td>\n",
       "      <td>7.767512</td>\n",
       "      <td>7.829657</td>\n",
       "      <td>8.395356</td>\n",
       "      <td>1.583711</td>\n",
       "      <td>10.125020</td>\n",
       "      <td>13.340874</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>1.962937</td>\n",
       "      <td>7.466666</td>\n",
       "      <td>11.547794</td>\n",
       "      <td>8.822916</td>\n",
       "      <td>9.046424</td>\n",
       "      <td>7.895535</td>\n",
       "      <td>11.010677</td>\n",
       "      <td>20.107472</td>\n",
       "      <td>55696</td>\n",
       "      <td>238144</td>\n",
       "      <td>256</td>\n",
       "      <td>48841</td>\n",
       "      <td>145924</td>\n",
       "      <td>9409</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>17424</td>\n",
       "      <td>78961</td>\n",
       "      <td>112896</td>\n",
       "      <td>9801</td>\n",
       "      <td>15.055162</td>\n",
       "      <td>3.231015</td>\n",
       "      <td>10.578597</td>\n",
       "      <td>147.184959</td>\n",
       "      <td>9.556511</td>\n",
       "      <td>31.700491</td>\n",
       "      <td>19.948487</td>\n",
       "      <td>6.306142</td>\n",
       "      <td>35.792987</td>\n",
       "      <td>20.622128</td>\n",
       "      <td>162.319736</td>\n",
       "      <td>159.641434</td>\n",
       "      <td>16.404880</td>\n",
       "      <td>139.883732</td>\n",
       "      <td>12.732913</td>\n",
       "      <td>180.110816</td>\n",
       "      <td>68.092313</td>\n",
       "      <td>5.234488</td>\n",
       "      <td>220.060315</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>8.853963</td>\n",
       "      <td>27.287592</td>\n",
       "      <td>1.469639</td>\n",
       "      <td>53.330838</td>\n",
       "      <td>50.171002</td>\n",
       "      <td>13.034627</td>\n",
       "      <td>60.334240</td>\n",
       "      <td>61.303533</td>\n",
       "      <td>70.482005</td>\n",
       "      <td>2.508141</td>\n",
       "      <td>102.516023</td>\n",
       "      <td>177.978912</td>\n",
       "      <td>0.757843</td>\n",
       "      <td>3.853122</td>\n",
       "      <td>55.751104</td>\n",
       "      <td>133.351552</td>\n",
       "      <td>77.843855</td>\n",
       "      <td>81.837795</td>\n",
       "      <td>62.339476</td>\n",
       "      <td>121.235018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>206</td>\n",
       "      <td>357</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>7.810250</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>109</td>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>12.099770</td>\n",
       "      <td>10.670550</td>\n",
       "      <td>14.137111</td>\n",
       "      <td>0.217037</td>\n",
       "      <td>1.426881</td>\n",
       "      <td>0.916617</td>\n",
       "      <td>8.168254</td>\n",
       "      <td>0.432319</td>\n",
       "      <td>5.872218</td>\n",
       "      <td>10.401401</td>\n",
       "      <td>12.843301</td>\n",
       "      <td>7.529992</td>\n",
       "      <td>12.525335</td>\n",
       "      <td>8.655344</td>\n",
       "      <td>8.499587</td>\n",
       "      <td>5.602552</td>\n",
       "      <td>4.187738</td>\n",
       "      <td>13.205982</td>\n",
       "      <td>5.432667</td>\n",
       "      <td>12.379275</td>\n",
       "      <td>11.938420</td>\n",
       "      <td>6.057282</td>\n",
       "      <td>2.581280</td>\n",
       "      <td>11.785456</td>\n",
       "      <td>4.445564</td>\n",
       "      <td>11.141346</td>\n",
       "      <td>0.839731</td>\n",
       "      <td>1.481908</td>\n",
       "      <td>8.920653</td>\n",
       "      <td>4.450379</td>\n",
       "      <td>10.584802</td>\n",
       "      <td>12.484882</td>\n",
       "      <td>7.168680</td>\n",
       "      <td>2.885415</td>\n",
       "      <td>12.413973</td>\n",
       "      <td>10.260494</td>\n",
       "      <td>10.091351</td>\n",
       "      <td>9.270888</td>\n",
       "      <td>3.173994</td>\n",
       "      <td>13.921871</td>\n",
       "      <td>61.763713</td>\n",
       "      <td>148996</td>\n",
       "      <td>42436</td>\n",
       "      <td>127449</td>\n",
       "      <td>53824</td>\n",
       "      <td>1</td>\n",
       "      <td>39204</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.583258</td>\n",
       "      <td>20449</td>\n",
       "      <td>11881</td>\n",
       "      <td>15129</td>\n",
       "      <td>16900</td>\n",
       "      <td>146.404444</td>\n",
       "      <td>113.860640</td>\n",
       "      <td>199.857919</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>2.035988</td>\n",
       "      <td>0.840188</td>\n",
       "      <td>66.720379</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>34.482939</td>\n",
       "      <td>108.189146</td>\n",
       "      <td>164.950377</td>\n",
       "      <td>56.700779</td>\n",
       "      <td>156.884013</td>\n",
       "      <td>74.914974</td>\n",
       "      <td>72.242985</td>\n",
       "      <td>31.388593</td>\n",
       "      <td>17.537152</td>\n",
       "      <td>174.397949</td>\n",
       "      <td>29.513869</td>\n",
       "      <td>153.246461</td>\n",
       "      <td>142.525869</td>\n",
       "      <td>36.690669</td>\n",
       "      <td>6.663006</td>\n",
       "      <td>138.896969</td>\n",
       "      <td>19.763037</td>\n",
       "      <td>124.129597</td>\n",
       "      <td>0.705147</td>\n",
       "      <td>2.196050</td>\n",
       "      <td>79.578053</td>\n",
       "      <td>19.805869</td>\n",
       "      <td>112.038042</td>\n",
       "      <td>155.872280</td>\n",
       "      <td>51.389978</td>\n",
       "      <td>8.325618</td>\n",
       "      <td>154.106737</td>\n",
       "      <td>105.277732</td>\n",
       "      <td>101.835356</td>\n",
       "      <td>85.949373</td>\n",
       "      <td>10.074237</td>\n",
       "      <td>193.818499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429</td>\n",
       "      <td>49</td>\n",
       "      <td>481</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>146</td>\n",
       "      <td>8.602325</td>\n",
       "      <td>0.651162</td>\n",
       "      <td>1</td>\n",
       "      <td>430</td>\n",
       "      <td>488</td>\n",
       "      <td>138</td>\n",
       "      <td>80</td>\n",
       "      <td>2.947479</td>\n",
       "      <td>12.671352</td>\n",
       "      <td>13.279918</td>\n",
       "      <td>13.163338</td>\n",
       "      <td>9.051826</td>\n",
       "      <td>11.605822</td>\n",
       "      <td>11.094161</td>\n",
       "      <td>10.461813</td>\n",
       "      <td>9.827713</td>\n",
       "      <td>2.206019</td>\n",
       "      <td>9.914789</td>\n",
       "      <td>4.448482</td>\n",
       "      <td>8.864810</td>\n",
       "      <td>10.837476</td>\n",
       "      <td>14.167872</td>\n",
       "      <td>13.456857</td>\n",
       "      <td>14.855511</td>\n",
       "      <td>7.596095</td>\n",
       "      <td>4.928033</td>\n",
       "      <td>2.439930</td>\n",
       "      <td>6.791165</td>\n",
       "      <td>8.709536</td>\n",
       "      <td>1.363087</td>\n",
       "      <td>4.980975</td>\n",
       "      <td>2.182372</td>\n",
       "      <td>14.673614</td>\n",
       "      <td>8.083289</td>\n",
       "      <td>6.601472</td>\n",
       "      <td>6.789200</td>\n",
       "      <td>12.982035</td>\n",
       "      <td>10.273114</td>\n",
       "      <td>14.030257</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>8.160625</td>\n",
       "      <td>12.592059</td>\n",
       "      <td>8.937577</td>\n",
       "      <td>2.265191</td>\n",
       "      <td>11.255721</td>\n",
       "      <td>12.794841</td>\n",
       "      <td>12.080951</td>\n",
       "      <td>74.651162</td>\n",
       "      <td>184041</td>\n",
       "      <td>2401</td>\n",
       "      <td>231361</td>\n",
       "      <td>12321</td>\n",
       "      <td>12321</td>\n",
       "      <td>21316</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.424012</td>\n",
       "      <td>184900</td>\n",
       "      <td>238144</td>\n",
       "      <td>19044</td>\n",
       "      <td>6400</td>\n",
       "      <td>8.687631</td>\n",
       "      <td>160.563165</td>\n",
       "      <td>176.356211</td>\n",
       "      <td>173.273479</td>\n",
       "      <td>81.935553</td>\n",
       "      <td>134.695100</td>\n",
       "      <td>123.080409</td>\n",
       "      <td>109.449526</td>\n",
       "      <td>96.583951</td>\n",
       "      <td>4.866522</td>\n",
       "      <td>98.303041</td>\n",
       "      <td>19.788989</td>\n",
       "      <td>78.584857</td>\n",
       "      <td>117.450880</td>\n",
       "      <td>200.728609</td>\n",
       "      <td>181.086992</td>\n",
       "      <td>220.686212</td>\n",
       "      <td>57.700659</td>\n",
       "      <td>24.285506</td>\n",
       "      <td>5.953258</td>\n",
       "      <td>46.119920</td>\n",
       "      <td>75.856011</td>\n",
       "      <td>1.858006</td>\n",
       "      <td>24.810113</td>\n",
       "      <td>4.762747</td>\n",
       "      <td>215.314961</td>\n",
       "      <td>65.339566</td>\n",
       "      <td>43.579435</td>\n",
       "      <td>46.093236</td>\n",
       "      <td>168.533225</td>\n",
       "      <td>105.536872</td>\n",
       "      <td>196.848111</td>\n",
       "      <td>0.156001</td>\n",
       "      <td>66.595795</td>\n",
       "      <td>158.559960</td>\n",
       "      <td>79.880277</td>\n",
       "      <td>5.131089</td>\n",
       "      <td>126.691249</td>\n",
       "      <td>163.707951</td>\n",
       "      <td>145.949365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414</td>\n",
       "      <td>350</td>\n",
       "      <td>481</td>\n",
       "      <td>370</td>\n",
       "      <td>208</td>\n",
       "      <td>158</td>\n",
       "      <td>8.306624</td>\n",
       "      <td>0.424645</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>111</td>\n",
       "      <td>38</td>\n",
       "      <td>177</td>\n",
       "      <td>5.368252</td>\n",
       "      <td>6.765946</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>7.175573</td>\n",
       "      <td>14.515096</td>\n",
       "      <td>1.901743</td>\n",
       "      <td>9.231263</td>\n",
       "      <td>9.600810</td>\n",
       "      <td>10.058844</td>\n",
       "      <td>5.680021</td>\n",
       "      <td>8.238473</td>\n",
       "      <td>0.272760</td>\n",
       "      <td>11.892743</td>\n",
       "      <td>4.030567</td>\n",
       "      <td>14.946749</td>\n",
       "      <td>7.121632</td>\n",
       "      <td>6.507572</td>\n",
       "      <td>12.462688</td>\n",
       "      <td>12.222522</td>\n",
       "      <td>0.318528</td>\n",
       "      <td>5.350321</td>\n",
       "      <td>3.143358</td>\n",
       "      <td>10.291804</td>\n",
       "      <td>13.105170</td>\n",
       "      <td>10.159100</td>\n",
       "      <td>3.671488</td>\n",
       "      <td>11.087198</td>\n",
       "      <td>1.289054</td>\n",
       "      <td>0.249375</td>\n",
       "      <td>2.967133</td>\n",
       "      <td>6.885179</td>\n",
       "      <td>2.789577</td>\n",
       "      <td>6.416708</td>\n",
       "      <td>10.549814</td>\n",
       "      <td>11.456437</td>\n",
       "      <td>6.468099</td>\n",
       "      <td>2.519049</td>\n",
       "      <td>0.258284</td>\n",
       "      <td>9.317696</td>\n",
       "      <td>5.383098</td>\n",
       "      <td>69.424645</td>\n",
       "      <td>171396</td>\n",
       "      <td>122500</td>\n",
       "      <td>231361</td>\n",
       "      <td>136900</td>\n",
       "      <td>43264</td>\n",
       "      <td>24964</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.180324</td>\n",
       "      <td>115600</td>\n",
       "      <td>12321</td>\n",
       "      <td>1444</td>\n",
       "      <td>31329</td>\n",
       "      <td>28.818126</td>\n",
       "      <td>45.778031</td>\n",
       "      <td>0.296388</td>\n",
       "      <td>51.488846</td>\n",
       "      <td>210.688010</td>\n",
       "      <td>3.616626</td>\n",
       "      <td>85.216219</td>\n",
       "      <td>92.175556</td>\n",
       "      <td>101.180337</td>\n",
       "      <td>32.262636</td>\n",
       "      <td>67.872439</td>\n",
       "      <td>0.074398</td>\n",
       "      <td>141.437342</td>\n",
       "      <td>16.245468</td>\n",
       "      <td>223.405315</td>\n",
       "      <td>50.717647</td>\n",
       "      <td>42.348491</td>\n",
       "      <td>155.318600</td>\n",
       "      <td>149.390054</td>\n",
       "      <td>0.101460</td>\n",
       "      <td>28.625935</td>\n",
       "      <td>9.880702</td>\n",
       "      <td>105.921234</td>\n",
       "      <td>171.745491</td>\n",
       "      <td>103.207308</td>\n",
       "      <td>13.479823</td>\n",
       "      <td>122.925953</td>\n",
       "      <td>1.661660</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>8.803876</td>\n",
       "      <td>47.405685</td>\n",
       "      <td>7.781742</td>\n",
       "      <td>41.174143</td>\n",
       "      <td>111.298565</td>\n",
       "      <td>131.249953</td>\n",
       "      <td>41.836302</td>\n",
       "      <td>6.345610</td>\n",
       "      <td>0.066711</td>\n",
       "      <td>86.819461</td>\n",
       "      <td>28.977739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>359</td>\n",
       "      <td>20</td>\n",
       "      <td>218</td>\n",
       "      <td>317</td>\n",
       "      <td>301</td>\n",
       "      <td>8.124038</td>\n",
       "      <td>0.767304</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>141</td>\n",
       "      <td>417</td>\n",
       "      <td>343</td>\n",
       "      <td>14.592218</td>\n",
       "      <td>6.483629</td>\n",
       "      <td>9.159313</td>\n",
       "      <td>5.083046</td>\n",
       "      <td>8.772015</td>\n",
       "      <td>7.687242</td>\n",
       "      <td>11.106926</td>\n",
       "      <td>13.926358</td>\n",
       "      <td>0.914295</td>\n",
       "      <td>1.418124</td>\n",
       "      <td>4.504223</td>\n",
       "      <td>6.158475</td>\n",
       "      <td>1.790923</td>\n",
       "      <td>7.049614</td>\n",
       "      <td>14.409808</td>\n",
       "      <td>11.615837</td>\n",
       "      <td>5.675790</td>\n",
       "      <td>9.136146</td>\n",
       "      <td>10.640432</td>\n",
       "      <td>14.051122</td>\n",
       "      <td>2.240417</td>\n",
       "      <td>3.271828</td>\n",
       "      <td>5.061455</td>\n",
       "      <td>3.679880</td>\n",
       "      <td>2.948615</td>\n",
       "      <td>6.419577</td>\n",
       "      <td>14.873100</td>\n",
       "      <td>14.806887</td>\n",
       "      <td>9.992545</td>\n",
       "      <td>3.701959</td>\n",
       "      <td>11.162686</td>\n",
       "      <td>1.886560</td>\n",
       "      <td>1.919999</td>\n",
       "      <td>2.268203</td>\n",
       "      <td>0.149421</td>\n",
       "      <td>4.105907</td>\n",
       "      <td>10.416291</td>\n",
       "      <td>6.816217</td>\n",
       "      <td>8.586960</td>\n",
       "      <td>4.512419</td>\n",
       "      <td>66.767304</td>\n",
       "      <td>101124</td>\n",
       "      <td>128881</td>\n",
       "      <td>400</td>\n",
       "      <td>47524</td>\n",
       "      <td>100489</td>\n",
       "      <td>90601</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.588755</td>\n",
       "      <td>44944</td>\n",
       "      <td>19881</td>\n",
       "      <td>173889</td>\n",
       "      <td>117649</td>\n",
       "      <td>212.932838</td>\n",
       "      <td>42.037442</td>\n",
       "      <td>83.893009</td>\n",
       "      <td>25.837360</td>\n",
       "      <td>76.948248</td>\n",
       "      <td>59.093690</td>\n",
       "      <td>123.363810</td>\n",
       "      <td>193.943450</td>\n",
       "      <td>0.835936</td>\n",
       "      <td>2.011074</td>\n",
       "      <td>20.288027</td>\n",
       "      <td>37.926811</td>\n",
       "      <td>3.207407</td>\n",
       "      <td>49.697063</td>\n",
       "      <td>207.642553</td>\n",
       "      <td>134.927678</td>\n",
       "      <td>32.214597</td>\n",
       "      <td>83.469158</td>\n",
       "      <td>113.218788</td>\n",
       "      <td>197.434018</td>\n",
       "      <td>5.019470</td>\n",
       "      <td>10.704858</td>\n",
       "      <td>25.618327</td>\n",
       "      <td>13.541518</td>\n",
       "      <td>8.694333</td>\n",
       "      <td>41.210963</td>\n",
       "      <td>221.209095</td>\n",
       "      <td>219.243898</td>\n",
       "      <td>99.850954</td>\n",
       "      <td>13.704500</td>\n",
       "      <td>124.605548</td>\n",
       "      <td>3.559110</td>\n",
       "      <td>3.686395</td>\n",
       "      <td>5.144746</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>16.858476</td>\n",
       "      <td>108.499122</td>\n",
       "      <td>46.460816</td>\n",
       "      <td>73.735875</td>\n",
       "      <td>20.361923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7  8    9   10   11   12  \\\n",
       "0  236  488   16  221  382   97 -4.472136  0.107472  0  132  281  336   99   \n",
       "1  386  206  357  232    1  198  7.810250  0.763713  1  143  109  123  130   \n",
       "2  429   49  481  111  111  146  8.602325  0.651162  1  430  488  138   80   \n",
       "3  414  350  481  370  208  158  8.306624  0.424645  1  340  111   38  177   \n",
       "4  318  359   20  218  317  301  8.124038  0.767304  1  212  141  417  343   \n",
       "\n",
       "          13         14         15         16         17         18  \\\n",
       "0   3.880098   1.797502   3.252475  12.131981   3.091361   5.630319   \n",
       "1  12.099770  10.670550  14.137111   0.217037   1.426881   0.916617   \n",
       "2   2.947479  12.671352  13.279918  13.163338   9.051826  11.605822   \n",
       "3   5.368252   6.765946   0.544415   7.175573  14.515096   1.901743   \n",
       "4  14.592218   6.483629   9.159313   5.083046   8.772015   7.687242   \n",
       "\n",
       "          19         20         21         22         23         24  \\\n",
       "0   4.466373   2.511203   5.982724   4.541159  12.740476  12.634929   \n",
       "1   8.168254   0.432319   5.872218  10.401401  12.843301   7.529992   \n",
       "2  11.094161  10.461813   9.827713   2.206019   9.914789   4.448482   \n",
       "3   9.231263   9.600810  10.058844   5.680021   8.238473   0.272760   \n",
       "4  11.106926  13.926358   0.914295   1.418124   4.504223   6.158475   \n",
       "\n",
       "          25         26         27         28         29         30  \\\n",
       "0   4.050294  11.827245   3.568321  13.420537   8.251807   2.287900   \n",
       "1  12.525335   8.655344   8.499587   5.602552   4.187738  13.205982   \n",
       "2   8.864810  10.837476  14.167872  13.456857  14.855511   7.596095   \n",
       "3  11.892743   4.030567  14.946749   7.121632   6.507572  12.462688   \n",
       "4   1.790923   7.049614  14.409808  11.615837   5.675790   9.136146   \n",
       "\n",
       "          31         32         33        34         35         36         37  \\\n",
       "0  14.834430   0.082253   2.975561  5.223753   1.212287   7.302797   7.083149   \n",
       "1   5.432667  12.379275  11.938420  6.057282   2.581280  11.785456   4.445564   \n",
       "2   4.928033   2.439930   6.791165  8.709536   1.363087   4.980975   2.182372   \n",
       "3  12.222522   0.318528   5.350321  3.143358  10.291804  13.105170  10.159100   \n",
       "4  10.640432  14.051122   2.240417  3.271828   5.061455   3.679880   2.948615   \n",
       "\n",
       "          38         39         40        41         42         43         44  \\\n",
       "0   3.610350   7.767512   7.829657  8.395356   1.583711  10.125020  13.340874   \n",
       "1  11.141346   0.839731   1.481908  8.920653   4.450379  10.584802  12.484882   \n",
       "2  14.673614   8.083289   6.601472  6.789200  12.982035  10.273114  14.030257   \n",
       "3   3.671488  11.087198   1.289054  0.249375   2.967133   6.885179   2.789577   \n",
       "4   6.419577  14.873100  14.806887  9.992545   3.701959  11.162686   1.886560   \n",
       "\n",
       "         45         46         47         48         49         50         51  \\\n",
       "0  0.870542   1.962937   7.466666  11.547794   8.822916   9.046424   7.895535   \n",
       "1  7.168680   2.885415  12.413973  10.260494  10.091351   9.270888   3.173994   \n",
       "2  0.394970   8.160625  12.592059   8.937577   2.265191  11.255721  12.794841   \n",
       "3  6.416708  10.549814  11.456437   6.468099   2.519049   0.258284   9.317696   \n",
       "4  1.919999   2.268203   0.149421   4.105907  10.416291   6.816217   8.586960   \n",
       "\n",
       "          52     target  0_square  1_square  2_square  3_square  4_square  \\\n",
       "0  11.010677  20.107472     55696    238144       256     48841    145924   \n",
       "1  13.921871  61.763713    148996     42436    127449     53824         1   \n",
       "2  12.080951  74.651162    184041      2401    231361     12321     12321   \n",
       "3   5.383098  69.424645    171396    122500    231361    136900     43264   \n",
       "4   4.512419  66.767304    101124    128881       400     47524    100489   \n",
       "\n",
       "   5_square  6_square  7_square  9_square  10_square  11_square  12_square  \\\n",
       "0      9409      20.0  0.011550     17424      78961     112896       9801   \n",
       "1     39204      61.0  0.583258     20449      11881      15129      16900   \n",
       "2     21316      74.0  0.424012    184900     238144      19044       6400   \n",
       "3     24964      69.0  0.180324    115600      12321       1444      31329   \n",
       "4     90601      66.0  0.588755     44944      19881     173889     117649   \n",
       "\n",
       "    13_square   14_square   15_square   16_square   17_square   18_square  \\\n",
       "0   15.055162    3.231015   10.578597  147.184959    9.556511   31.700491   \n",
       "1  146.404444  113.860640  199.857919    0.047105    2.035988    0.840188   \n",
       "2    8.687631  160.563165  176.356211  173.273479   81.935553  134.695100   \n",
       "3   28.818126   45.778031    0.296388   51.488846  210.688010    3.616626   \n",
       "4  212.932838   42.037442   83.893009   25.837360   76.948248   59.093690   \n",
       "\n",
       "    19_square   20_square   21_square   22_square   23_square   24_square  \\\n",
       "0   19.948487    6.306142   35.792987   20.622128  162.319736  159.641434   \n",
       "1   66.720379    0.186899   34.482939  108.189146  164.950377   56.700779   \n",
       "2  123.080409  109.449526   96.583951    4.866522   98.303041   19.788989   \n",
       "3   85.216219   92.175556  101.180337   32.262636   67.872439    0.074398   \n",
       "4  123.363810  193.943450    0.835936    2.011074   20.288027   37.926811   \n",
       "\n",
       "    25_square   26_square   27_square   28_square   29_square   30_square  \\\n",
       "0   16.404880  139.883732   12.732913  180.110816   68.092313    5.234488   \n",
       "1  156.884013   74.914974   72.242985   31.388593   17.537152  174.397949   \n",
       "2   78.584857  117.450880  200.728609  181.086992  220.686212   57.700659   \n",
       "3  141.437342   16.245468  223.405315   50.717647   42.348491  155.318600   \n",
       "4    3.207407   49.697063  207.642553  134.927678   32.214597   83.469158   \n",
       "\n",
       "    31_square   32_square   33_square  34_square   35_square   36_square  \\\n",
       "0  220.060315    0.006766    8.853963  27.287592    1.469639   53.330838   \n",
       "1   29.513869  153.246461  142.525869  36.690669    6.663006  138.896969   \n",
       "2   24.285506    5.953258   46.119920  75.856011    1.858006   24.810113   \n",
       "3  149.390054    0.101460   28.625935   9.880702  105.921234  171.745491   \n",
       "4  113.218788  197.434018    5.019470  10.704858   25.618327   13.541518   \n",
       "\n",
       "    37_square   38_square   39_square   40_square  41_square   42_square  \\\n",
       "0   50.171002   13.034627   60.334240   61.303533  70.482005    2.508141   \n",
       "1   19.763037  124.129597    0.705147    2.196050  79.578053   19.805869   \n",
       "2    4.762747  215.314961   65.339566   43.579435  46.093236  168.533225   \n",
       "3  103.207308   13.479823  122.925953    1.661660   0.062188    8.803876   \n",
       "4    8.694333   41.210963  221.209095  219.243898  99.850954   13.704500   \n",
       "\n",
       "    43_square   44_square  45_square   46_square   47_square   48_square  \\\n",
       "0  102.516023  177.978912   0.757843    3.853122   55.751104  133.351552   \n",
       "1  112.038042  155.872280  51.389978    8.325618  154.106737  105.277732   \n",
       "2  105.536872  196.848111   0.156001   66.595795  158.559960   79.880277   \n",
       "3   47.405685    7.781742  41.174143  111.298565  131.249953   41.836302   \n",
       "4  124.605548    3.559110   3.686395    5.144746    0.022327   16.858476   \n",
       "\n",
       "    49_square   50_square   51_square   52_square  \n",
       "0   77.843855   81.837795   62.339476  121.235018  \n",
       "1  101.835356   85.949373   10.074237  193.818499  \n",
       "2    5.131089  126.691249  163.707951  145.949365  \n",
       "3    6.345610    0.066711   86.819461   28.977739  \n",
       "4  108.499122   46.460816   73.735875   20.361923  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in data_exp_columns_sq:\n",
    "    sq_name = c + '_square'\n",
    "    dataset_exp[sq_name] = np.square(dataset_exp[c])\n",
    "    dataset_exp_columns.append(sq_name)\n",
    "    \n",
    "dataset_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation all columns ok. We scale target value also.\n",
    "scaling_feature = dataset_exp_columns\n",
    "dataset_scaling = scaler.fit_transform(dataset_exp[scaling_feature])\n",
    "dataset_scaling = pd.DataFrame(dataset_scaling)\n",
    "dataset_scaling.columns = dataset_exp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "      <th>0_square</th>\n",
       "      <th>1_square</th>\n",
       "      <th>2_square</th>\n",
       "      <th>3_square</th>\n",
       "      <th>4_square</th>\n",
       "      <th>5_square</th>\n",
       "      <th>6_square</th>\n",
       "      <th>7_square</th>\n",
       "      <th>9_square</th>\n",
       "      <th>10_square</th>\n",
       "      <th>11_square</th>\n",
       "      <th>12_square</th>\n",
       "      <th>13_square</th>\n",
       "      <th>14_square</th>\n",
       "      <th>15_square</th>\n",
       "      <th>16_square</th>\n",
       "      <th>17_square</th>\n",
       "      <th>18_square</th>\n",
       "      <th>19_square</th>\n",
       "      <th>20_square</th>\n",
       "      <th>21_square</th>\n",
       "      <th>22_square</th>\n",
       "      <th>23_square</th>\n",
       "      <th>24_square</th>\n",
       "      <th>25_square</th>\n",
       "      <th>26_square</th>\n",
       "      <th>27_square</th>\n",
       "      <th>28_square</th>\n",
       "      <th>29_square</th>\n",
       "      <th>30_square</th>\n",
       "      <th>31_square</th>\n",
       "      <th>32_square</th>\n",
       "      <th>33_square</th>\n",
       "      <th>34_square</th>\n",
       "      <th>35_square</th>\n",
       "      <th>36_square</th>\n",
       "      <th>37_square</th>\n",
       "      <th>38_square</th>\n",
       "      <th>39_square</th>\n",
       "      <th>40_square</th>\n",
       "      <th>41_square</th>\n",
       "      <th>42_square</th>\n",
       "      <th>43_square</th>\n",
       "      <th>44_square</th>\n",
       "      <th>45_square</th>\n",
       "      <th>46_square</th>\n",
       "      <th>47_square</th>\n",
       "      <th>48_square</th>\n",
       "      <th>49_square</th>\n",
       "      <th>50_square</th>\n",
       "      <th>51_square</th>\n",
       "      <th>52_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472946</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.442886</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.194389</td>\n",
       "      <td>0.275267</td>\n",
       "      <td>0.107460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264529</td>\n",
       "      <td>0.563126</td>\n",
       "      <td>0.673347</td>\n",
       "      <td>0.198397</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.216827</td>\n",
       "      <td>0.808797</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.375349</td>\n",
       "      <td>0.297760</td>\n",
       "      <td>0.167393</td>\n",
       "      <td>0.398846</td>\n",
       "      <td>0.302742</td>\n",
       "      <td>0.849368</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.270017</td>\n",
       "      <td>0.788483</td>\n",
       "      <td>0.237887</td>\n",
       "      <td>0.894704</td>\n",
       "      <td>0.550113</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.198367</td>\n",
       "      <td>0.348258</td>\n",
       "      <td>0.080815</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.472212</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.105564</td>\n",
       "      <td>0.675018</td>\n",
       "      <td>0.889397</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>0.497792</td>\n",
       "      <td>0.769854</td>\n",
       "      <td>0.588197</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.526387</td>\n",
       "      <td>0.734054</td>\n",
       "      <td>0.201055</td>\n",
       "      <td>0.223678</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.196148</td>\n",
       "      <td>0.586038</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.069976</td>\n",
       "      <td>0.317111</td>\n",
       "      <td>0.453396</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.654157</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>0.140894</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.028027</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.091657</td>\n",
       "      <td>0.721433</td>\n",
       "      <td>0.709519</td>\n",
       "      <td>0.072914</td>\n",
       "      <td>0.621706</td>\n",
       "      <td>0.056592</td>\n",
       "      <td>0.800497</td>\n",
       "      <td>0.302639</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.978055</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.039352</td>\n",
       "      <td>0.121285</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.237026</td>\n",
       "      <td>0.222988</td>\n",
       "      <td>0.057932</td>\n",
       "      <td>0.268155</td>\n",
       "      <td>0.272471</td>\n",
       "      <td>0.313256</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.455651</td>\n",
       "      <td>0.791028</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.592684</td>\n",
       "      <td>0.345976</td>\n",
       "      <td>0.363726</td>\n",
       "      <td>0.277088</td>\n",
       "      <td>0.538838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773547</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>0.464930</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.892480</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.218437</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.260521</td>\n",
       "      <td>0.806654</td>\n",
       "      <td>0.711368</td>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.391479</td>\n",
       "      <td>0.693435</td>\n",
       "      <td>0.856224</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.577023</td>\n",
       "      <td>0.566643</td>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.279165</td>\n",
       "      <td>0.880410</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.825290</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.403828</td>\n",
       "      <td>0.172082</td>\n",
       "      <td>0.785697</td>\n",
       "      <td>0.296370</td>\n",
       "      <td>0.742758</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>0.098789</td>\n",
       "      <td>0.594705</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.705671</td>\n",
       "      <td>0.832331</td>\n",
       "      <td>0.477926</td>\n",
       "      <td>0.192356</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.684031</td>\n",
       "      <td>0.672760</td>\n",
       "      <td>0.618057</td>\n",
       "      <td>0.211601</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.617630</td>\n",
       "      <td>0.598375</td>\n",
       "      <td>0.170425</td>\n",
       "      <td>0.511841</td>\n",
       "      <td>0.216160</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.157445</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.583273</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>0.047715</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.650693</td>\n",
       "      <td>0.506051</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.009049</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.296550</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.153268</td>\n",
       "      <td>0.480856</td>\n",
       "      <td>0.733125</td>\n",
       "      <td>0.252004</td>\n",
       "      <td>0.697295</td>\n",
       "      <td>0.332956</td>\n",
       "      <td>0.321087</td>\n",
       "      <td>0.139506</td>\n",
       "      <td>0.077945</td>\n",
       "      <td>0.775122</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.681105</td>\n",
       "      <td>0.633459</td>\n",
       "      <td>0.163079</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.617320</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.551690</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.353683</td>\n",
       "      <td>0.088029</td>\n",
       "      <td>0.497973</td>\n",
       "      <td>0.692775</td>\n",
       "      <td>0.228414</td>\n",
       "      <td>0.037004</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.467909</td>\n",
       "      <td>0.452606</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.044778</td>\n",
       "      <td>0.861441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859719</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.292585</td>\n",
       "      <td>0.932283</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.276553</td>\n",
       "      <td>0.160321</td>\n",
       "      <td>0.196495</td>\n",
       "      <td>0.844758</td>\n",
       "      <td>0.885346</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.603460</td>\n",
       "      <td>0.773725</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.697449</td>\n",
       "      <td>0.655194</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>0.660983</td>\n",
       "      <td>0.296560</td>\n",
       "      <td>0.590996</td>\n",
       "      <td>0.722498</td>\n",
       "      <td>0.944535</td>\n",
       "      <td>0.897125</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.506411</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>0.162659</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>0.580651</td>\n",
       "      <td>0.090868</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>0.538885</td>\n",
       "      <td>0.440103</td>\n",
       "      <td>0.452605</td>\n",
       "      <td>0.865481</td>\n",
       "      <td>0.684891</td>\n",
       "      <td>0.935357</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.839499</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.151013</td>\n",
       "      <td>0.750381</td>\n",
       "      <td>0.853024</td>\n",
       "      <td>0.805407</td>\n",
       "      <td>0.746509</td>\n",
       "      <td>0.739118</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.929157</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.085606</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.424023</td>\n",
       "      <td>0.742567</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.076482</td>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.038612</td>\n",
       "      <td>0.713620</td>\n",
       "      <td>0.783841</td>\n",
       "      <td>0.770106</td>\n",
       "      <td>0.364166</td>\n",
       "      <td>0.598656</td>\n",
       "      <td>0.547051</td>\n",
       "      <td>0.486445</td>\n",
       "      <td>0.429292</td>\n",
       "      <td>0.021630</td>\n",
       "      <td>0.436910</td>\n",
       "      <td>0.087951</td>\n",
       "      <td>0.349282</td>\n",
       "      <td>0.522004</td>\n",
       "      <td>0.892146</td>\n",
       "      <td>0.804836</td>\n",
       "      <td>0.980848</td>\n",
       "      <td>0.256454</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.204981</td>\n",
       "      <td>0.337158</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.110267</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>0.956961</td>\n",
       "      <td>0.290401</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>0.204860</td>\n",
       "      <td>0.749063</td>\n",
       "      <td>0.469077</td>\n",
       "      <td>0.874892</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.295992</td>\n",
       "      <td>0.704760</td>\n",
       "      <td>0.355030</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.563076</td>\n",
       "      <td>0.727653</td>\n",
       "      <td>0.648683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.963928</td>\n",
       "      <td>0.741483</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.316633</td>\n",
       "      <td>0.917424</td>\n",
       "      <td>0.424643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681363</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.076152</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.357881</td>\n",
       "      <td>0.451056</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>0.478366</td>\n",
       "      <td>0.967683</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>0.615429</td>\n",
       "      <td>0.640047</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.378668</td>\n",
       "      <td>0.549225</td>\n",
       "      <td>0.018176</td>\n",
       "      <td>0.792865</td>\n",
       "      <td>0.268704</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.433826</td>\n",
       "      <td>0.830856</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.356686</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.686123</td>\n",
       "      <td>0.873678</td>\n",
       "      <td>0.677279</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.739148</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.185972</td>\n",
       "      <td>0.427792</td>\n",
       "      <td>0.703330</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.431196</td>\n",
       "      <td>0.167937</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.621202</td>\n",
       "      <td>0.358873</td>\n",
       "      <td>0.694242</td>\n",
       "      <td>0.688335</td>\n",
       "      <td>0.491966</td>\n",
       "      <td>0.929157</td>\n",
       "      <td>0.549797</td>\n",
       "      <td>0.173750</td>\n",
       "      <td>0.100257</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.464255</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.125819</td>\n",
       "      <td>0.128082</td>\n",
       "      <td>0.203460</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.228840</td>\n",
       "      <td>0.936411</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.378757</td>\n",
       "      <td>0.409672</td>\n",
       "      <td>0.449721</td>\n",
       "      <td>0.143394</td>\n",
       "      <td>0.301660</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.628640</td>\n",
       "      <td>0.072202</td>\n",
       "      <td>0.992934</td>\n",
       "      <td>0.225413</td>\n",
       "      <td>0.188220</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.663962</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.127229</td>\n",
       "      <td>0.043917</td>\n",
       "      <td>0.470767</td>\n",
       "      <td>0.763314</td>\n",
       "      <td>0.458710</td>\n",
       "      <td>0.059911</td>\n",
       "      <td>0.546342</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.210703</td>\n",
       "      <td>0.034586</td>\n",
       "      <td>0.183008</td>\n",
       "      <td>0.494678</td>\n",
       "      <td>0.583374</td>\n",
       "      <td>0.185942</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.385897</td>\n",
       "      <td>0.128794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637275</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.436874</td>\n",
       "      <td>0.635271</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.908248</td>\n",
       "      <td>0.767310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.687375</td>\n",
       "      <td>0.972820</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.610630</td>\n",
       "      <td>0.338862</td>\n",
       "      <td>0.584805</td>\n",
       "      <td>0.512480</td>\n",
       "      <td>0.740478</td>\n",
       "      <td>0.928425</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.094535</td>\n",
       "      <td>0.300266</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>0.119386</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.960664</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.378372</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.936748</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.218125</td>\n",
       "      <td>0.337429</td>\n",
       "      <td>0.245324</td>\n",
       "      <td>0.196572</td>\n",
       "      <td>0.427972</td>\n",
       "      <td>0.991544</td>\n",
       "      <td>0.987145</td>\n",
       "      <td>0.666166</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.744198</td>\n",
       "      <td>0.125770</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.694422</td>\n",
       "      <td>0.454410</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>0.667668</td>\n",
       "      <td>0.406119</td>\n",
       "      <td>0.517592</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.190859</td>\n",
       "      <td>0.403569</td>\n",
       "      <td>0.363858</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.588770</td>\n",
       "      <td>0.180497</td>\n",
       "      <td>0.079843</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>0.472484</td>\n",
       "      <td>0.946378</td>\n",
       "      <td>0.186835</td>\n",
       "      <td>0.372875</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.262644</td>\n",
       "      <td>0.548310</td>\n",
       "      <td>0.861976</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>0.168564</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.220876</td>\n",
       "      <td>0.922876</td>\n",
       "      <td>0.599682</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>0.370984</td>\n",
       "      <td>0.503199</td>\n",
       "      <td>0.877497</td>\n",
       "      <td>0.022309</td>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.113861</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.983160</td>\n",
       "      <td>0.974456</td>\n",
       "      <td>0.443786</td>\n",
       "      <td>0.060911</td>\n",
       "      <td>0.553832</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.022866</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>0.482223</td>\n",
       "      <td>0.206494</td>\n",
       "      <td>0.327743</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.140281</td>\n",
       "      <td>0.599198</td>\n",
       "      <td>0.595190</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.857715</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.793016</td>\n",
       "      <td>0.505189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.146293</td>\n",
       "      <td>0.807615</td>\n",
       "      <td>0.248497</td>\n",
       "      <td>0.983968</td>\n",
       "      <td>0.512097</td>\n",
       "      <td>0.944259</td>\n",
       "      <td>0.879559</td>\n",
       "      <td>0.841348</td>\n",
       "      <td>0.084555</td>\n",
       "      <td>0.945598</td>\n",
       "      <td>0.861795</td>\n",
       "      <td>0.636221</td>\n",
       "      <td>0.254937</td>\n",
       "      <td>0.496932</td>\n",
       "      <td>0.072116</td>\n",
       "      <td>0.397040</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.568529</td>\n",
       "      <td>0.811183</td>\n",
       "      <td>0.391630</td>\n",
       "      <td>0.147034</td>\n",
       "      <td>0.064446</td>\n",
       "      <td>0.055786</td>\n",
       "      <td>0.570349</td>\n",
       "      <td>0.398927</td>\n",
       "      <td>0.739345</td>\n",
       "      <td>0.195662</td>\n",
       "      <td>0.264680</td>\n",
       "      <td>0.430097</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.130483</td>\n",
       "      <td>0.593502</td>\n",
       "      <td>0.419949</td>\n",
       "      <td>0.380904</td>\n",
       "      <td>0.637835</td>\n",
       "      <td>0.099124</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.749723</td>\n",
       "      <td>0.407752</td>\n",
       "      <td>0.621061</td>\n",
       "      <td>0.270881</td>\n",
       "      <td>0.477110</td>\n",
       "      <td>0.808848</td>\n",
       "      <td>0.942754</td>\n",
       "      <td>0.345036</td>\n",
       "      <td>0.019679</td>\n",
       "      <td>0.359039</td>\n",
       "      <td>0.354252</td>\n",
       "      <td>0.491966</td>\n",
       "      <td>0.735676</td>\n",
       "      <td>0.517592</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.255223</td>\n",
       "      <td>0.021402</td>\n",
       "      <td>0.652242</td>\n",
       "      <td>0.061751</td>\n",
       "      <td>0.968193</td>\n",
       "      <td>0.262247</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.773627</td>\n",
       "      <td>0.707870</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.894158</td>\n",
       "      <td>0.742692</td>\n",
       "      <td>0.404789</td>\n",
       "      <td>0.065003</td>\n",
       "      <td>0.246946</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.157645</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.323226</td>\n",
       "      <td>0.658019</td>\n",
       "      <td>0.153379</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.159146</td>\n",
       "      <td>0.546633</td>\n",
       "      <td>0.038285</td>\n",
       "      <td>0.070056</td>\n",
       "      <td>0.184986</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.017027</td>\n",
       "      <td>0.352248</td>\n",
       "      <td>0.176366</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.406836</td>\n",
       "      <td>0.009826</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.562088</td>\n",
       "      <td>0.166264</td>\n",
       "      <td>0.385729</td>\n",
       "      <td>0.073377</td>\n",
       "      <td>0.227639</td>\n",
       "      <td>0.654239</td>\n",
       "      <td>0.888786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.915832</td>\n",
       "      <td>0.727455</td>\n",
       "      <td>0.324649</td>\n",
       "      <td>0.134269</td>\n",
       "      <td>0.777555</td>\n",
       "      <td>0.653307</td>\n",
       "      <td>0.428933</td>\n",
       "      <td>0.604311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517034</td>\n",
       "      <td>0.903808</td>\n",
       "      <td>0.438878</td>\n",
       "      <td>0.621242</td>\n",
       "      <td>0.791226</td>\n",
       "      <td>0.884668</td>\n",
       "      <td>0.837140</td>\n",
       "      <td>0.782075</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.540169</td>\n",
       "      <td>0.242425</td>\n",
       "      <td>0.164233</td>\n",
       "      <td>0.089240</td>\n",
       "      <td>0.553726</td>\n",
       "      <td>0.845871</td>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.983199</td>\n",
       "      <td>0.751756</td>\n",
       "      <td>0.184059</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.558025</td>\n",
       "      <td>0.526312</td>\n",
       "      <td>0.992436</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>0.543738</td>\n",
       "      <td>0.397279</td>\n",
       "      <td>0.347586</td>\n",
       "      <td>0.612617</td>\n",
       "      <td>0.994940</td>\n",
       "      <td>0.105207</td>\n",
       "      <td>0.974982</td>\n",
       "      <td>0.845950</td>\n",
       "      <td>0.156352</td>\n",
       "      <td>0.740216</td>\n",
       "      <td>0.165010</td>\n",
       "      <td>0.550689</td>\n",
       "      <td>0.349330</td>\n",
       "      <td>0.077564</td>\n",
       "      <td>0.719841</td>\n",
       "      <td>0.387925</td>\n",
       "      <td>0.296742</td>\n",
       "      <td>0.976358</td>\n",
       "      <td>0.782689</td>\n",
       "      <td>0.984216</td>\n",
       "      <td>0.026018</td>\n",
       "      <td>0.838748</td>\n",
       "      <td>0.529191</td>\n",
       "      <td>0.105397</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>0.604592</td>\n",
       "      <td>0.426810</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.365199</td>\n",
       "      <td>0.267324</td>\n",
       "      <td>0.816868</td>\n",
       "      <td>0.192614</td>\n",
       "      <td>0.385942</td>\n",
       "      <td>0.626041</td>\n",
       "      <td>0.782641</td>\n",
       "      <td>0.700807</td>\n",
       "      <td>0.611646</td>\n",
       "      <td>0.234181</td>\n",
       "      <td>0.291790</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.306617</td>\n",
       "      <td>0.715504</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.966681</td>\n",
       "      <td>0.565137</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.311406</td>\n",
       "      <td>0.277006</td>\n",
       "      <td>0.984929</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.295655</td>\n",
       "      <td>0.157833</td>\n",
       "      <td>0.120818</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>0.950589</td>\n",
       "      <td>0.715633</td>\n",
       "      <td>0.024451</td>\n",
       "      <td>0.547927</td>\n",
       "      <td>0.027229</td>\n",
       "      <td>0.303258</td>\n",
       "      <td>0.122033</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>0.518174</td>\n",
       "      <td>0.150498</td>\n",
       "      <td>0.088056</td>\n",
       "      <td>0.953275</td>\n",
       "      <td>0.612605</td>\n",
       "      <td>0.968681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.829659</td>\n",
       "      <td>0.543086</td>\n",
       "      <td>0.326653</td>\n",
       "      <td>0.635271</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.232465</td>\n",
       "      <td>0.182179</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971944</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.997996</td>\n",
       "      <td>0.783567</td>\n",
       "      <td>0.069664</td>\n",
       "      <td>0.322048</td>\n",
       "      <td>0.900062</td>\n",
       "      <td>0.413603</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>0.510129</td>\n",
       "      <td>0.951280</td>\n",
       "      <td>0.934250</td>\n",
       "      <td>0.529932</td>\n",
       "      <td>0.704864</td>\n",
       "      <td>0.589219</td>\n",
       "      <td>0.280241</td>\n",
       "      <td>0.710346</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.213034</td>\n",
       "      <td>0.237852</td>\n",
       "      <td>0.730591</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.497321</td>\n",
       "      <td>0.952616</td>\n",
       "      <td>0.823127</td>\n",
       "      <td>0.707840</td>\n",
       "      <td>0.751852</td>\n",
       "      <td>0.041329</td>\n",
       "      <td>0.186635</td>\n",
       "      <td>0.027628</td>\n",
       "      <td>0.773302</td>\n",
       "      <td>0.569415</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.762411</td>\n",
       "      <td>0.395006</td>\n",
       "      <td>0.405345</td>\n",
       "      <td>0.754575</td>\n",
       "      <td>0.734277</td>\n",
       "      <td>0.809156</td>\n",
       "      <td>0.183984</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.282927</td>\n",
       "      <td>0.215499</td>\n",
       "      <td>0.400211</td>\n",
       "      <td>0.688335</td>\n",
       "      <td>0.294943</td>\n",
       "      <td>0.106702</td>\n",
       "      <td>0.403569</td>\n",
       "      <td>0.517592</td>\n",
       "      <td>0.054040</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.944675</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.995996</td>\n",
       "      <td>0.613977</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.103722</td>\n",
       "      <td>0.810114</td>\n",
       "      <td>0.171074</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.638142</td>\n",
       "      <td>0.260236</td>\n",
       "      <td>0.904935</td>\n",
       "      <td>0.872827</td>\n",
       "      <td>0.280833</td>\n",
       "      <td>0.496844</td>\n",
       "      <td>0.347183</td>\n",
       "      <td>0.078540</td>\n",
       "      <td>0.504591</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.045387</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.533764</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.247331</td>\n",
       "      <td>0.907479</td>\n",
       "      <td>0.677540</td>\n",
       "      <td>0.501039</td>\n",
       "      <td>0.565282</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.034833</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.597999</td>\n",
       "      <td>0.324242</td>\n",
       "      <td>0.203278</td>\n",
       "      <td>0.581272</td>\n",
       "      <td>0.156031</td>\n",
       "      <td>0.164306</td>\n",
       "      <td>0.569388</td>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.654742</td>\n",
       "      <td>0.033851</td>\n",
       "      <td>0.493511</td>\n",
       "      <td>0.080052</td>\n",
       "      <td>0.046442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.841683</td>\n",
       "      <td>0.170341</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.142285</td>\n",
       "      <td>0.486974</td>\n",
       "      <td>0.603206</td>\n",
       "      <td>0.067717</td>\n",
       "      <td>0.228063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.803607</td>\n",
       "      <td>0.621242</td>\n",
       "      <td>0.613226</td>\n",
       "      <td>0.617234</td>\n",
       "      <td>0.183953</td>\n",
       "      <td>0.316217</td>\n",
       "      <td>0.275716</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.511723</td>\n",
       "      <td>0.931969</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>0.465086</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.696824</td>\n",
       "      <td>0.166910</td>\n",
       "      <td>0.186624</td>\n",
       "      <td>0.085850</td>\n",
       "      <td>0.985362</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>0.927856</td>\n",
       "      <td>0.507517</td>\n",
       "      <td>0.027708</td>\n",
       "      <td>0.012394</td>\n",
       "      <td>0.167161</td>\n",
       "      <td>0.708813</td>\n",
       "      <td>0.250281</td>\n",
       "      <td>0.229897</td>\n",
       "      <td>0.272980</td>\n",
       "      <td>0.484184</td>\n",
       "      <td>0.500670</td>\n",
       "      <td>0.701340</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.419825</td>\n",
       "      <td>0.174962</td>\n",
       "      <td>0.365546</td>\n",
       "      <td>0.102294</td>\n",
       "      <td>0.312681</td>\n",
       "      <td>0.742828</td>\n",
       "      <td>0.819358</td>\n",
       "      <td>0.131667</td>\n",
       "      <td>0.737447</td>\n",
       "      <td>0.965830</td>\n",
       "      <td>0.882958</td>\n",
       "      <td>0.742278</td>\n",
       "      <td>0.708431</td>\n",
       "      <td>0.029016</td>\n",
       "      <td>0.573829</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.237144</td>\n",
       "      <td>0.363858</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.052018</td>\n",
       "      <td>0.645785</td>\n",
       "      <td>0.385942</td>\n",
       "      <td>0.376047</td>\n",
       "      <td>0.380978</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.076024</td>\n",
       "      <td>0.050942</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.261868</td>\n",
       "      <td>0.868567</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.216319</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.485574</td>\n",
       "      <td>0.027861</td>\n",
       "      <td>0.034832</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.970939</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>0.860920</td>\n",
       "      <td>0.257575</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.027945</td>\n",
       "      <td>0.502417</td>\n",
       "      <td>0.062642</td>\n",
       "      <td>0.052853</td>\n",
       "      <td>0.074520</td>\n",
       "      <td>0.234435</td>\n",
       "      <td>0.250674</td>\n",
       "      <td>0.491881</td>\n",
       "      <td>0.796741</td>\n",
       "      <td>0.176263</td>\n",
       "      <td>0.030613</td>\n",
       "      <td>0.133624</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.097774</td>\n",
       "      <td>0.551796</td>\n",
       "      <td>0.671354</td>\n",
       "      <td>0.017336</td>\n",
       "      <td>0.543832</td>\n",
       "      <td>0.932829</td>\n",
       "      <td>0.779617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.891784</td>\n",
       "      <td>0.609218</td>\n",
       "      <td>0.719439</td>\n",
       "      <td>0.597194</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.739479</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.919395</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.126253</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.939880</td>\n",
       "      <td>0.448898</td>\n",
       "      <td>0.734655</td>\n",
       "      <td>0.618327</td>\n",
       "      <td>0.989675</td>\n",
       "      <td>0.093778</td>\n",
       "      <td>0.971495</td>\n",
       "      <td>0.498254</td>\n",
       "      <td>0.820590</td>\n",
       "      <td>0.397147</td>\n",
       "      <td>0.474223</td>\n",
       "      <td>0.784309</td>\n",
       "      <td>0.245555</td>\n",
       "      <td>0.619133</td>\n",
       "      <td>0.181624</td>\n",
       "      <td>0.020636</td>\n",
       "      <td>0.378241</td>\n",
       "      <td>0.510298</td>\n",
       "      <td>0.220646</td>\n",
       "      <td>0.167853</td>\n",
       "      <td>0.370235</td>\n",
       "      <td>0.786370</td>\n",
       "      <td>0.667954</td>\n",
       "      <td>0.664664</td>\n",
       "      <td>0.323545</td>\n",
       "      <td>0.998259</td>\n",
       "      <td>0.946380</td>\n",
       "      <td>0.640444</td>\n",
       "      <td>0.486972</td>\n",
       "      <td>0.468680</td>\n",
       "      <td>0.832985</td>\n",
       "      <td>0.447284</td>\n",
       "      <td>0.913437</td>\n",
       "      <td>0.817850</td>\n",
       "      <td>0.333306</td>\n",
       "      <td>0.025117</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.060939</td>\n",
       "      <td>0.831446</td>\n",
       "      <td>0.917349</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.305572</td>\n",
       "      <td>0.779192</td>\n",
       "      <td>0.795278</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>0.517592</td>\n",
       "      <td>0.356641</td>\n",
       "      <td>0.079843</td>\n",
       "      <td>0.546829</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.845289</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.691664</td>\n",
       "      <td>0.883374</td>\n",
       "      <td>0.201509</td>\n",
       "      <td>0.539720</td>\n",
       "      <td>0.382337</td>\n",
       "      <td>0.979458</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.943803</td>\n",
       "      <td>0.248264</td>\n",
       "      <td>0.673371</td>\n",
       "      <td>0.157738</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.615143</td>\n",
       "      <td>0.060307</td>\n",
       "      <td>0.383329</td>\n",
       "      <td>0.032991</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>0.260409</td>\n",
       "      <td>0.048695</td>\n",
       "      <td>0.028175</td>\n",
       "      <td>0.137076</td>\n",
       "      <td>0.618380</td>\n",
       "      <td>0.446166</td>\n",
       "      <td>0.441781</td>\n",
       "      <td>0.104683</td>\n",
       "      <td>0.996522</td>\n",
       "      <td>0.895636</td>\n",
       "      <td>0.410169</td>\n",
       "      <td>0.237145</td>\n",
       "      <td>0.219665</td>\n",
       "      <td>0.693869</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.834367</td>\n",
       "      <td>0.668879</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.691303</td>\n",
       "      <td>0.841531</td>\n",
       "      <td>0.934559</td>\n",
       "      <td>0.093378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.472946  0.977956  0.032064  0.442886  0.765531  0.194389  0.275267   \n",
       "1  0.773547  0.412826  0.715431  0.464930  0.002004  0.396794  0.892480   \n",
       "2  0.859719  0.098196  0.963928  0.222445  0.222445  0.292585  0.932283   \n",
       "3  0.829659  0.701403  0.963928  0.741483  0.416834  0.316633  0.917424   \n",
       "4  0.637275  0.719439  0.040080  0.436874  0.635271  0.603206  0.908248   \n",
       "5  0.140281  0.599198  0.595190  0.701403  0.857715  0.719439  0.793016   \n",
       "6  0.915832  0.727455  0.324649  0.134269  0.777555  0.653307  0.428933   \n",
       "7  0.829659  0.543086  0.326653  0.635271  0.719439  0.232465  0.182179   \n",
       "8  0.841683  0.170341  0.757515  0.142285  0.486974  0.603206  0.067717   \n",
       "9  0.891784  0.609218  0.719439  0.597194  0.282565  0.739479  0.940959   \n",
       "\n",
       "          7    8         9        10        11        12        13        14  \\\n",
       "0  0.107460  0.0  0.264529  0.563126  0.673347  0.198397  0.258670  0.119819   \n",
       "1  0.763720  1.0  0.286573  0.218437  0.246493  0.260521  0.806654  0.711368   \n",
       "2  0.651165  1.0  0.861723  0.977956  0.276553  0.160321  0.196495  0.844758   \n",
       "3  0.424643  1.0  0.681363  0.222445  0.076152  0.354709  0.357881  0.451056   \n",
       "4  0.767310  1.0  0.424850  0.282565  0.835671  0.687375  0.972820  0.432234   \n",
       "5  0.505189  1.0  0.146293  0.807615  0.248497  0.983968  0.512097  0.944259   \n",
       "6  0.604311  0.0  0.517034  0.903808  0.438878  0.621242  0.791226  0.884668   \n",
       "7  0.022450  0.0  0.971944  0.196393  0.997996  0.783567  0.069664  0.322048   \n",
       "8  0.228063  0.0  0.803607  0.621242  0.613226  0.617234  0.183953  0.316217   \n",
       "9  0.919395  1.0  0.126253  0.831663  0.939880  0.448898  0.734655  0.618327   \n",
       "\n",
       "         15        16        17        18        19        20        21  \\\n",
       "0  0.216827  0.808797  0.206089  0.375349  0.297760  0.167393  0.398846   \n",
       "1  0.942495  0.014457  0.095122  0.061094  0.544560  0.028797  0.391479   \n",
       "2  0.885346  0.877556  0.603460  0.773725  0.739627  0.697449  0.655194   \n",
       "3  0.036283  0.478366  0.967683  0.126771  0.615429  0.640047  0.670604   \n",
       "4  0.610630  0.338862  0.584805  0.512480  0.740478  0.928425  0.060930   \n",
       "5  0.879559  0.841348  0.084555  0.945598  0.861795  0.636221  0.254937   \n",
       "6  0.837140  0.782075  0.483920  0.540169  0.242425  0.164233  0.089240   \n",
       "7  0.900062  0.413603  0.016428  0.798835  0.510129  0.951280  0.934250   \n",
       "8  0.275716  0.225694  0.007493  0.511723  0.931969  0.023646  0.465086   \n",
       "9  0.989675  0.093778  0.971495  0.498254  0.820590  0.397147  0.474223   \n",
       "\n",
       "         22        23        24        25        26        27        28  \\\n",
       "0  0.302742  0.849368  0.842328  0.270017  0.788483  0.237887  0.894704   \n",
       "1  0.693435  0.856224  0.501996  0.835040  0.577023  0.566643  0.373498   \n",
       "2  0.147062  0.660983  0.296560  0.590996  0.722498  0.944535  0.897125   \n",
       "3  0.378668  0.549225  0.018176  0.792865  0.268704  0.996461  0.474771   \n",
       "4  0.094535  0.300266  0.410560  0.119386  0.469974  0.960664  0.774389   \n",
       "5  0.496932  0.072116  0.397040  0.070370  0.568529  0.811183  0.391630   \n",
       "6  0.553726  0.845871  0.088175  0.983199  0.751756  0.184059  0.025178   \n",
       "7  0.529932  0.704864  0.589219  0.280241  0.710346  0.046725  0.213034   \n",
       "8  0.057007  0.696824  0.166910  0.186624  0.085850  0.985362  0.129139   \n",
       "9  0.784309  0.245555  0.619133  0.181624  0.020636  0.378241  0.510298   \n",
       "\n",
       "         29        30        31        32        33        34        35  \\\n",
       "0  0.550113  0.152526  0.988967  0.005479  0.198367  0.348258  0.080815   \n",
       "1  0.279165  0.880410  0.362177  0.825290  0.795900  0.403828  0.172082   \n",
       "2  0.990378  0.506411  0.328534  0.162659  0.452744  0.580651  0.090868   \n",
       "3  0.433826  0.830856  0.814838  0.021231  0.356686  0.209560  0.686123   \n",
       "4  0.378372  0.609083  0.709364  0.936748  0.149356  0.218125  0.337429   \n",
       "5  0.147034  0.064446  0.055786  0.570349  0.398927  0.739345  0.195662   \n",
       "6  0.558025  0.526312  0.992436  0.093413  0.543738  0.397279  0.347586   \n",
       "7  0.237852  0.730591  0.014757  0.497321  0.952616  0.823127  0.707840   \n",
       "8  0.927856  0.507517  0.027708  0.012394  0.167161  0.708813  0.250281   \n",
       "9  0.220646  0.167853  0.370235  0.786370  0.667954  0.664664  0.323545   \n",
       "\n",
       "         36        37        38        39        40        41        42  \\\n",
       "0  0.486853  0.472212  0.240690  0.517833  0.521984  0.559685  0.105564   \n",
       "1  0.785697  0.296370  0.742758  0.055976  0.098789  0.594705  0.296682   \n",
       "2  0.332064  0.145488  0.978244  0.538885  0.440103  0.452605  0.865481   \n",
       "3  0.873678  0.677279  0.244766  0.739148  0.085932  0.016607  0.197795   \n",
       "4  0.245324  0.196572  0.427972  0.991544  0.987145  0.666166  0.246786   \n",
       "5  0.264680  0.430097  0.154200  0.130483  0.593502  0.419949  0.380904   \n",
       "6  0.612617  0.994940  0.105207  0.974982  0.845950  0.156352  0.740216   \n",
       "7  0.751852  0.041329  0.186635  0.027628  0.773302  0.569415  0.450852   \n",
       "8  0.229897  0.272980  0.484184  0.500670  0.701340  0.892601  0.419825   \n",
       "9  0.998259  0.946380  0.640444  0.486972  0.468680  0.832985  0.447284   \n",
       "\n",
       "         43        44        45        46        47        48        49  \\\n",
       "0  0.675018  0.889397  0.058035  0.130856  0.497792  0.769854  0.588197   \n",
       "1  0.705671  0.832331  0.477926  0.192356  0.827626  0.684031  0.672760   \n",
       "2  0.684891  0.935357  0.026330  0.544047  0.839499  0.595834  0.151013   \n",
       "3  0.459022  0.185972  0.427792  0.703330  0.763788  0.431196  0.167937   \n",
       "4  0.744198  0.125770  0.128002  0.151208  0.009956  0.273712  0.694422   \n",
       "5  0.637835  0.099124  0.024407  0.749723  0.407752  0.621061  0.270881   \n",
       "6  0.165010  0.550689  0.349330  0.077564  0.719841  0.387925  0.296742   \n",
       "7  0.762411  0.395006  0.405345  0.754575  0.734277  0.809156  0.183984   \n",
       "8  0.174962  0.365546  0.102294  0.312681  0.742828  0.819358  0.131667   \n",
       "9  0.913437  0.817850  0.333306  0.025117  0.008114  0.060939  0.831446   \n",
       "\n",
       "         50        51        52    target  0_square  1_square  2_square  \\\n",
       "0  0.603093  0.526387  0.734054  0.201055  0.223678  0.956398  0.001028   \n",
       "1  0.618057  0.211601  0.928138  0.617630  0.598375  0.170425  0.511841   \n",
       "2  0.750381  0.853024  0.805407  0.746509  0.739118  0.009643  0.929157   \n",
       "3  0.017208  0.621202  0.358873  0.694242  0.688335  0.491966  0.929157   \n",
       "4  0.454410  0.572484  0.300827  0.667668  0.406119  0.517592  0.001606   \n",
       "5  0.477110  0.808848  0.942754  0.345036  0.019679  0.359039  0.354252   \n",
       "6  0.976358  0.782689  0.984216  0.026018  0.838748  0.529191  0.105397   \n",
       "7  0.702500  0.282927  0.215499  0.400211  0.688335  0.294943  0.106702   \n",
       "8  0.737447  0.965830  0.882958  0.742278  0.708431  0.029016  0.573829   \n",
       "9  0.917349  0.966726  0.305572  0.779192  0.795278  0.371147  0.517592   \n",
       "\n",
       "   3_square  4_square  5_square  6_square  7_square  9_square  10_square  \\\n",
       "0  0.196148  0.586038  0.037787  0.202020  0.011550  0.069976   0.317111   \n",
       "1  0.216160  0.000004  0.157445  0.616162  0.583273  0.082124   0.047715   \n",
       "2  0.049482  0.049482  0.085606  0.747475  0.424023  0.742567   0.956398   \n",
       "3  0.549797  0.173750  0.100257  0.696970  0.180328  0.464255   0.049482   \n",
       "4  0.190859  0.403569  0.363858  0.666667  0.588770  0.180497   0.079843   \n",
       "5  0.491966  0.735676  0.517592  0.343434  0.255223  0.021402   0.652242   \n",
       "6  0.018028  0.604592  0.426810  0.020202  0.365199  0.267324   0.816868   \n",
       "7  0.403569  0.517592  0.054040  0.404040  0.000505  0.944675   0.038570   \n",
       "8  0.020245  0.237144  0.363858  0.747475  0.052018  0.645785   0.385942   \n",
       "9  0.356641  0.079843  0.546829  0.777778  0.845289  0.015940   0.691664   \n",
       "\n",
       "   11_square  12_square  13_square  14_square  15_square  16_square  \\\n",
       "0   0.453396   0.039361   0.066913   0.014360   0.047018   0.654157   \n",
       "1   0.060759   0.067871   0.650693   0.506051   0.888298   0.000209   \n",
       "2   0.076482   0.025703   0.038612   0.713620   0.783841   0.770106   \n",
       "3   0.005799   0.125819   0.128082   0.203460   0.001317   0.228840   \n",
       "4   0.698347   0.472484   0.946378   0.186835   0.372875   0.114833   \n",
       "5   0.061751   0.968193   0.262247   0.891626   0.773627   0.707870   \n",
       "6   0.192614   0.385942   0.626041   0.782641   0.700807   0.611646   \n",
       "7   0.995996   0.613977   0.004854   0.103722   0.810114   0.171074   \n",
       "8   0.376047   0.380978   0.033841   0.100000   0.076024   0.050942   \n",
       "9   0.883374   0.201509   0.539720   0.382337   0.979458   0.008797   \n",
       "\n",
       "   17_square  18_square  19_square  20_square  21_square  22_square  \\\n",
       "0   0.042474   0.140894   0.088664   0.028027   0.159091   0.091657   \n",
       "1   0.009049   0.003734   0.296550   0.000831   0.153268   0.480856   \n",
       "2   0.364166   0.598656   0.547051   0.486445   0.429292   0.021630   \n",
       "3   0.936411   0.016074   0.378757   0.409672   0.449721   0.143394   \n",
       "4   0.342000   0.262644   0.548310   0.861976   0.003716   0.008938   \n",
       "5   0.007150   0.894158   0.742692   0.404789   0.065003   0.246946   \n",
       "6   0.234181   0.291790   0.058773   0.026980   0.007968   0.306617   \n",
       "7   0.000270   0.638142   0.260236   0.904935   0.872827   0.280833   \n",
       "8   0.000056   0.261868   0.868567   0.000560   0.216319   0.003251   \n",
       "9   0.943803   0.248264   0.673371   0.157738   0.224900   0.615143   \n",
       "\n",
       "   23_square  24_square  25_square  26_square  27_square  28_square  \\\n",
       "0   0.721433   0.709519   0.072914   0.621706   0.056592   0.800497   \n",
       "1   0.733125   0.252004   0.697295   0.332956   0.321087   0.139506   \n",
       "2   0.436910   0.087951   0.349282   0.522004   0.892146   0.804836   \n",
       "3   0.301660   0.000331   0.628640   0.072202   0.992934   0.225413   \n",
       "4   0.090171   0.168564   0.014256   0.220876   0.922876   0.599682   \n",
       "5   0.005204   0.157645   0.004954   0.323226   0.658019   0.153379   \n",
       "6   0.715504   0.007776   0.966681   0.565137   0.033879   0.000634   \n",
       "7   0.496844   0.347183   0.078540   0.504591   0.002184   0.045387   \n",
       "8   0.485574   0.027861   0.034832   0.007370   0.970939   0.016679   \n",
       "9   0.060307   0.383329   0.032991   0.000426   0.143069   0.260409   \n",
       "\n",
       "   29_square  30_square  31_square  32_square  33_square  34_square  \\\n",
       "0   0.302639   0.023265   0.978055   0.000030   0.039352   0.121285   \n",
       "1   0.077945   0.775122   0.131174   0.681105   0.633459   0.163079   \n",
       "2   0.980848   0.256454   0.107937   0.026459   0.204981   0.337158   \n",
       "3   0.188220   0.690323   0.663962   0.000451   0.127229   0.043917   \n",
       "4   0.143179   0.370984   0.503199   0.877497   0.022309   0.047580   \n",
       "5   0.021626   0.004154   0.003113   0.325300   0.159146   0.546633   \n",
       "6   0.311406   0.277006   0.984929   0.008727   0.295655   0.157833   \n",
       "7   0.056584   0.533764   0.000218   0.247331   0.907479   0.677540   \n",
       "8   0.860920   0.257575   0.000768   0.000154   0.027945   0.502417   \n",
       "9   0.048695   0.028175   0.137076   0.618380   0.446166   0.441781   \n",
       "\n",
       "   35_square  36_square  37_square  38_square  39_square  40_square  \\\n",
       "0   0.006532   0.237026   0.222988   0.057932   0.268155   0.272471   \n",
       "1   0.029614   0.617320   0.087838   0.551690   0.003134   0.009761   \n",
       "2   0.008258   0.110267   0.021168   0.956961   0.290401   0.193694   \n",
       "3   0.470767   0.763314   0.458710   0.059911   0.546342   0.007385   \n",
       "4   0.113861   0.060185   0.038642   0.183161   0.983160   0.974456   \n",
       "5   0.038285   0.070056   0.184986   0.023778   0.017027   0.352248   \n",
       "6   0.120818   0.375300   0.989905   0.011069   0.950589   0.715633   \n",
       "7   0.501039   0.565282   0.001709   0.034833   0.000764   0.597999   \n",
       "8   0.062642   0.052853   0.074520   0.234435   0.250674   0.491881   \n",
       "9   0.104683   0.996522   0.895636   0.410169   0.237145   0.219665   \n",
       "\n",
       "   41_square  42_square  43_square  44_square  45_square  46_square  \\\n",
       "0   0.313256   0.011148   0.455651   0.791028   0.003368   0.017126   \n",
       "1   0.353683   0.088029   0.497973   0.692775   0.228414   0.037004   \n",
       "2   0.204860   0.749063   0.469077   0.874892   0.000693   0.295992   \n",
       "3   0.000276   0.039130   0.210703   0.034586   0.183008   0.494678   \n",
       "4   0.443786   0.060911   0.553832   0.015818   0.016385   0.022866   \n",
       "5   0.176366   0.145098   0.406836   0.009826   0.000596   0.562088   \n",
       "6   0.024451   0.547927   0.027229   0.303258   0.122033   0.006018   \n",
       "7   0.324242   0.203278   0.581272   0.156031   0.164306   0.569388   \n",
       "8   0.796741   0.176263   0.030613   0.133624   0.010465   0.097774   \n",
       "9   0.693869   0.200074   0.834367   0.668879   0.111094   0.000631   \n",
       "\n",
       "   47_square  48_square  49_square  50_square  51_square  52_square  \n",
       "0   0.247800   0.592684   0.345976   0.363726   0.277088   0.538838  \n",
       "1   0.684966   0.467909   0.452606   0.382000   0.044778   0.861441  \n",
       "2   0.704760   0.355030   0.022805   0.563076   0.727653   0.648683  \n",
       "3   0.583374   0.185942   0.028203   0.000296   0.385897   0.128794  \n",
       "4   0.000099   0.074928   0.482223   0.206494   0.327743   0.090500  \n",
       "5   0.166264   0.385729   0.073377   0.227639   0.654239   0.888786  \n",
       "6   0.518174   0.150498   0.088056   0.953275   0.612605   0.968681  \n",
       "7   0.539164   0.654742   0.033851   0.493511   0.080052   0.046442  \n",
       "8   0.551796   0.671354   0.017336   0.543832   0.932829   0.779617  \n",
       "9   0.000066   0.003716   0.691303   0.841531   0.934559   0.093378  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scaling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data on X and y\n",
    "X_data = dataset_scaling.loc[:, dataset_scaling.columns!='target']\n",
    "y_data = dataset_scaling['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " 'target',\n",
       " '0_square',\n",
       " '1_square',\n",
       " '2_square',\n",
       " '3_square',\n",
       " '4_square',\n",
       " '5_square',\n",
       " '6_square',\n",
       " '7_square',\n",
       " '9_square',\n",
       " '10_square',\n",
       " '11_square',\n",
       " '12_square',\n",
       " '13_square',\n",
       " '14_square',\n",
       " '15_square',\n",
       " '16_square',\n",
       " '17_square',\n",
       " '18_square',\n",
       " '19_square',\n",
       " '20_square',\n",
       " '21_square',\n",
       " '22_square',\n",
       " '23_square',\n",
       " '24_square',\n",
       " '25_square',\n",
       " '26_square',\n",
       " '27_square',\n",
       " '28_square',\n",
       " '29_square',\n",
       " '30_square',\n",
       " '31_square',\n",
       " '32_square',\n",
       " '33_square',\n",
       " '34_square',\n",
       " '35_square',\n",
       " '36_square',\n",
       " '37_square',\n",
       " '38_square',\n",
       " '39_square',\n",
       " '40_square',\n",
       " '41_square',\n",
       " '42_square',\n",
       " '43_square',\n",
       " '44_square',\n",
       " '45_square',\n",
       " '46_square',\n",
       " '47_square',\n",
       " '48_square',\n",
       " '49_square',\n",
       " '50_square',\n",
       " '51_square',\n",
       " '52_square']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_exp_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.0\n",
      "RMSE scores: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data, y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")\n",
    "# result_df['LinearRegression_Multi6_8'] = [np.around(abs(rmse_scores), 5), np.around(abs(np.mean(rmse_scores)), 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have zero RMSE. That means our prediction is ideal. So some prediction make the best fitiing to target. But which one? So, my next step find feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=50)\n",
    "rf.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Feature Importance')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sort = rf.feature_importances_.argsort()\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.barh(X_data.columns, rf.feature_importances_)\n",
    "plt.xlabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, one feature has the biggest importance for model. And its 6_square. Let's look into df of importances, maybe we find another non-zero predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_square</th>\n",
       "      <td>0.99990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_square</th>\n",
       "      <td>0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_square</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52_square</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Importance\n",
       "6_square      0.99990\n",
       "7             0.00005\n",
       "7_square      0.00005\n",
       "0             0.00000\n",
       "15_square     0.00000\n",
       "...               ...\n",
       "32            0.00000\n",
       "31            0.00000\n",
       "30            0.00000\n",
       "29            0.00000\n",
       "52_square     0.00000\n",
       "\n",
       "[105 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp_df = pd.DataFrame({'Importance': np.round(rf.feature_importances_, 5)},\n",
    "                             index=X_data.columns)\n",
    "feature_imp_df.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model have influence from only 3 feature. Let's try predict on this 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Three feature Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_predictors = ['6_square', '7', '7_square']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.0\n",
      "RMSE scores: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Train only on 3 important predictors\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data[important_predictors], y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 5)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, we can remove one more feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin with ['6_square', '7', '7_square']\n",
      "Remove:  7\n",
      "RMSE scores mean: 0.0007232585\n",
      "RMSE scores: [0.00072506 0.0007267  0.0007233  0.00072179 0.00071945]\n",
      "********************\n",
      "After Remove cols ['6_square', '7_square']\n",
      "Back cols 7\n",
      "We have ['6_square', '7_square', '7']\n",
      "----------\n",
      "Begin with ['6_square', '7_square', '7']\n",
      "Remove:  7_square\n",
      "RMSE scores mean: 0.0\n",
      "RMSE scores: [0. 0. 0. 0. 0.]\n",
      "********************\n",
      "After Remove cols ['6_square', '7']\n",
      "Back cols 7_square\n",
      "We have ['6_square', '7', '7_square']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Try to remove predictors with 7's(because 6_square have impotances 0.99, if delete it model will predict obviously worse)\n",
    "cols = important_predictors.copy()\n",
    "for preds in ['7', '7_square']:\n",
    "    print('Begin with', cols)\n",
    "    print('Remove: ',preds)\n",
    "    cols.remove(preds)\n",
    "    model = LinearRegression()\n",
    "    rmse_scores = np.array(cross_val_score(model, X_data[cols], y_data, scoring='neg_root_mean_squared_error'))\n",
    "    print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 10)}\")\n",
    "    print(f\"RMSE scores: {np.around(abs(rmse_scores), 10)}\")\n",
    "    print('*'*20)\n",
    "    print('After Remove cols', cols)\n",
    "    print('Back cols', preds)\n",
    "    cols.append(preds)\n",
    "    print('We have', cols)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, according to result, we can remove 7_square feature, because after deleting, RMSE = 0 -> noting change to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores mean: 0.0\n",
      "RMSE scores: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# train on left features \n",
    "left_features = ['6_square', '7']\n",
    "model = LinearRegression()\n",
    "rmse_scores = np.array(cross_val_score(model, X_data[left_features], y_data, scoring='neg_root_mean_squared_error'))\n",
    "print(f\"RMSE scores mean: {np.around(abs(np.mean(rmse_scores)), 10)}\")\n",
    "print(f\"RMSE scores: {np.around(abs(rmse_scores), 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_data[left_features], y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6_square</th>\n",
       "      <th>7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.107460</td>\n",
       "      <td>0.201055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.763720</td>\n",
       "      <td>0.617630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>0.746509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.424643</td>\n",
       "      <td>0.694242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.767310</td>\n",
       "      <td>0.667668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6_square         7    target\n",
       "0  0.202020  0.107460  0.201055\n",
       "1  0.616162  0.763720  0.617630\n",
       "2  0.747475  0.651165  0.746509\n",
       "3  0.696970  0.424643  0.694242\n",
       "4  0.666667  0.767310  0.667668"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at dataset with left predictors\n",
    "dataset_scaling[['6_square', '7', 'target']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 6_square give almost exactly result as a target have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Predict Test Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Upload Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>388</td>\n",
       "      <td>402</td>\n",
       "      <td>340</td>\n",
       "      <td>156</td>\n",
       "      <td>382</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.218760</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>183</td>\n",
       "      <td>411</td>\n",
       "      <td>239</td>\n",
       "      <td>1.636870</td>\n",
       "      <td>4.316116</td>\n",
       "      <td>3.087304</td>\n",
       "      <td>12.844492</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.643383</td>\n",
       "      <td>3.104759</td>\n",
       "      <td>5.579529</td>\n",
       "      <td>7.633182</td>\n",
       "      <td>9.056701</td>\n",
       "      <td>8.621463</td>\n",
       "      <td>4.039640</td>\n",
       "      <td>1.191639</td>\n",
       "      <td>2.775038</td>\n",
       "      <td>9.728365</td>\n",
       "      <td>6.701686</td>\n",
       "      <td>11.456474</td>\n",
       "      <td>3.380898</td>\n",
       "      <td>4.866598</td>\n",
       "      <td>6.705850</td>\n",
       "      <td>2.521471</td>\n",
       "      <td>3.124903</td>\n",
       "      <td>12.787034</td>\n",
       "      <td>7.653819</td>\n",
       "      <td>2.067580</td>\n",
       "      <td>14.527098</td>\n",
       "      <td>13.773822</td>\n",
       "      <td>6.570842</td>\n",
       "      <td>7.280782</td>\n",
       "      <td>9.876851</td>\n",
       "      <td>7.163474</td>\n",
       "      <td>10.227235</td>\n",
       "      <td>0.889111</td>\n",
       "      <td>9.190222</td>\n",
       "      <td>4.670908</td>\n",
       "      <td>0.931525</td>\n",
       "      <td>7.634177</td>\n",
       "      <td>0.158196</td>\n",
       "      <td>13.432551</td>\n",
       "      <td>2.511191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>192</td>\n",
       "      <td>381</td>\n",
       "      <td>421</td>\n",
       "      <td>452</td>\n",
       "      <td>123</td>\n",
       "      <td>-8.888194</td>\n",
       "      <td>0.700228</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>229</td>\n",
       "      <td>420</td>\n",
       "      <td>476</td>\n",
       "      <td>6.256282</td>\n",
       "      <td>0.410825</td>\n",
       "      <td>0.693713</td>\n",
       "      <td>5.895766</td>\n",
       "      <td>7.309578</td>\n",
       "      <td>2.315738</td>\n",
       "      <td>6.204979</td>\n",
       "      <td>4.264013</td>\n",
       "      <td>11.956969</td>\n",
       "      <td>1.801893</td>\n",
       "      <td>13.581027</td>\n",
       "      <td>7.460250</td>\n",
       "      <td>7.883021</td>\n",
       "      <td>10.537323</td>\n",
       "      <td>12.286389</td>\n",
       "      <td>4.499934</td>\n",
       "      <td>3.302528</td>\n",
       "      <td>1.490149</td>\n",
       "      <td>10.010222</td>\n",
       "      <td>2.469233</td>\n",
       "      <td>9.206524</td>\n",
       "      <td>3.097949</td>\n",
       "      <td>14.556454</td>\n",
       "      <td>10.811960</td>\n",
       "      <td>14.584917</td>\n",
       "      <td>2.021892</td>\n",
       "      <td>7.956624</td>\n",
       "      <td>3.006465</td>\n",
       "      <td>10.685368</td>\n",
       "      <td>5.047971</td>\n",
       "      <td>10.619064</td>\n",
       "      <td>9.339161</td>\n",
       "      <td>4.221861</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>13.865748</td>\n",
       "      <td>0.328453</td>\n",
       "      <td>11.507599</td>\n",
       "      <td>9.107966</td>\n",
       "      <td>1.302407</td>\n",
       "      <td>11.105604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>129</td>\n",
       "      <td>107</td>\n",
       "      <td>156</td>\n",
       "      <td>247</td>\n",
       "      <td>191</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.494556</td>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>448</td>\n",
       "      <td>420</td>\n",
       "      <td>428</td>\n",
       "      <td>5.724503</td>\n",
       "      <td>12.465191</td>\n",
       "      <td>1.977254</td>\n",
       "      <td>7.652835</td>\n",
       "      <td>2.649014</td>\n",
       "      <td>14.531226</td>\n",
       "      <td>1.293337</td>\n",
       "      <td>9.192163</td>\n",
       "      <td>7.218093</td>\n",
       "      <td>9.580598</td>\n",
       "      <td>10.240284</td>\n",
       "      <td>12.522093</td>\n",
       "      <td>11.639480</td>\n",
       "      <td>11.349518</td>\n",
       "      <td>10.959723</td>\n",
       "      <td>12.820784</td>\n",
       "      <td>9.176369</td>\n",
       "      <td>10.157479</td>\n",
       "      <td>4.107533</td>\n",
       "      <td>7.454075</td>\n",
       "      <td>12.178675</td>\n",
       "      <td>9.401233</td>\n",
       "      <td>10.292276</td>\n",
       "      <td>3.530603</td>\n",
       "      <td>12.680169</td>\n",
       "      <td>10.921196</td>\n",
       "      <td>8.008255</td>\n",
       "      <td>14.940933</td>\n",
       "      <td>4.579603</td>\n",
       "      <td>14.150860</td>\n",
       "      <td>1.819890</td>\n",
       "      <td>10.670237</td>\n",
       "      <td>10.350867</td>\n",
       "      <td>5.134417</td>\n",
       "      <td>5.898995</td>\n",
       "      <td>8.374986</td>\n",
       "      <td>4.638049</td>\n",
       "      <td>3.160023</td>\n",
       "      <td>2.243799</td>\n",
       "      <td>5.073030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "      <td>438</td>\n",
       "      <td>191</td>\n",
       "      <td>116</td>\n",
       "      <td>491</td>\n",
       "      <td>403</td>\n",
       "      <td>-6.164414</td>\n",
       "      <td>0.511117</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>315</td>\n",
       "      <td>23</td>\n",
       "      <td>421</td>\n",
       "      <td>10.868374</td>\n",
       "      <td>8.335662</td>\n",
       "      <td>3.561422</td>\n",
       "      <td>9.939135</td>\n",
       "      <td>4.885376</td>\n",
       "      <td>7.706857</td>\n",
       "      <td>4.214658</td>\n",
       "      <td>10.346181</td>\n",
       "      <td>2.590524</td>\n",
       "      <td>1.511884</td>\n",
       "      <td>2.357567</td>\n",
       "      <td>5.231946</td>\n",
       "      <td>4.697991</td>\n",
       "      <td>11.870758</td>\n",
       "      <td>0.455004</td>\n",
       "      <td>0.645782</td>\n",
       "      <td>7.849633</td>\n",
       "      <td>5.334536</td>\n",
       "      <td>5.262470</td>\n",
       "      <td>9.251172</td>\n",
       "      <td>9.335831</td>\n",
       "      <td>9.708345</td>\n",
       "      <td>6.091409</td>\n",
       "      <td>6.789830</td>\n",
       "      <td>1.552119</td>\n",
       "      <td>12.569473</td>\n",
       "      <td>7.002166</td>\n",
       "      <td>4.999168</td>\n",
       "      <td>14.278131</td>\n",
       "      <td>0.650444</td>\n",
       "      <td>4.168135</td>\n",
       "      <td>12.782579</td>\n",
       "      <td>0.513072</td>\n",
       "      <td>0.321295</td>\n",
       "      <td>11.334062</td>\n",
       "      <td>11.735511</td>\n",
       "      <td>1.911520</td>\n",
       "      <td>8.365676</td>\n",
       "      <td>4.877288</td>\n",
       "      <td>11.601819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>449</td>\n",
       "      <td>156</td>\n",
       "      <td>310</td>\n",
       "      <td>188</td>\n",
       "      <td>279</td>\n",
       "      <td>465</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.756416</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>74</td>\n",
       "      <td>481</td>\n",
       "      <td>55</td>\n",
       "      <td>11.871029</td>\n",
       "      <td>10.237341</td>\n",
       "      <td>14.890562</td>\n",
       "      <td>4.589386</td>\n",
       "      <td>12.968020</td>\n",
       "      <td>4.907581</td>\n",
       "      <td>14.461897</td>\n",
       "      <td>13.128528</td>\n",
       "      <td>4.062875</td>\n",
       "      <td>2.253413</td>\n",
       "      <td>12.471074</td>\n",
       "      <td>7.078105</td>\n",
       "      <td>0.843648</td>\n",
       "      <td>14.961904</td>\n",
       "      <td>6.157512</td>\n",
       "      <td>11.370193</td>\n",
       "      <td>12.969449</td>\n",
       "      <td>7.130421</td>\n",
       "      <td>8.974373</td>\n",
       "      <td>6.173560</td>\n",
       "      <td>12.682708</td>\n",
       "      <td>7.864171</td>\n",
       "      <td>2.075725</td>\n",
       "      <td>14.339978</td>\n",
       "      <td>3.365456</td>\n",
       "      <td>6.278199</td>\n",
       "      <td>9.302832</td>\n",
       "      <td>9.715296</td>\n",
       "      <td>14.142186</td>\n",
       "      <td>9.875013</td>\n",
       "      <td>7.908122</td>\n",
       "      <td>1.140498</td>\n",
       "      <td>14.165074</td>\n",
       "      <td>7.688796</td>\n",
       "      <td>5.079241</td>\n",
       "      <td>8.152186</td>\n",
       "      <td>1.680403</td>\n",
       "      <td>13.215111</td>\n",
       "      <td>5.823109</td>\n",
       "      <td>1.038015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7  8    9   10   11   12  \\\n",
       "0  259  388  402  340  156  382  3.316625  0.218760  1  164  183  411  239   \n",
       "1  441  192  381  421  452  123 -8.888194  0.700228  0  349  229  420  476   \n",
       "2   83  129  107  156  247  191  2.645751  0.494556  1  297  448  420  428   \n",
       "3  183  438  191  116  491  403 -6.164414  0.511117  0  272  315   23  421   \n",
       "4  449  156  310  188  279  465  8.000000  0.756416  1   90   74  481   55   \n",
       "\n",
       "          13         14         15         16         17         18  \\\n",
       "0   1.636870   4.316116   3.087304  12.844492   0.054046   0.643383   \n",
       "1   6.256282   0.410825   0.693713   5.895766   7.309578   2.315738   \n",
       "2   5.724503  12.465191   1.977254   7.652835   2.649014  14.531226   \n",
       "3  10.868374   8.335662   3.561422   9.939135   4.885376   7.706857   \n",
       "4  11.871029  10.237341  14.890562   4.589386  12.968020   4.907581   \n",
       "\n",
       "          19         20         21        22         23         24         25  \\\n",
       "0   3.104759   5.579529   7.633182  9.056701   8.621463   4.039640   1.191639   \n",
       "1   6.204979   4.264013  11.956969  1.801893  13.581027   7.460250   7.883021   \n",
       "2   1.293337   9.192163   7.218093  9.580598  10.240284  12.522093  11.639480   \n",
       "3   4.214658  10.346181   2.590524  1.511884   2.357567   5.231946   4.697991   \n",
       "4  14.461897  13.128528   4.062875  2.253413  12.471074   7.078105   0.843648   \n",
       "\n",
       "          26         27         28         29         30         31        32  \\\n",
       "0   2.775038   9.728365   6.701686  11.456474   3.380898   4.866598  6.705850   \n",
       "1  10.537323  12.286389   4.499934   3.302528   1.490149  10.010222  2.469233   \n",
       "2  11.349518  10.959723  12.820784   9.176369  10.157479   4.107533  7.454075   \n",
       "3  11.870758   0.455004   0.645782   7.849633   5.334536   5.262470  9.251172   \n",
       "4  14.961904   6.157512  11.370193  12.969449   7.130421   8.974373  6.173560   \n",
       "\n",
       "          33        34         35         36         37         38         39  \\\n",
       "0   2.521471  3.124903  12.787034   7.653819   2.067580  14.527098  13.773822   \n",
       "1   9.206524  3.097949  14.556454  10.811960  14.584917   2.021892   7.956624   \n",
       "2  12.178675  9.401233  10.292276   3.530603  12.680169  10.921196   8.008255   \n",
       "3   9.335831  9.708345   6.091409   6.789830   1.552119  12.569473   7.002166   \n",
       "4  12.682708  7.864171   2.075725  14.339978   3.365456   6.278199   9.302832   \n",
       "\n",
       "          40         41         42         43         44         45        46  \\\n",
       "0   6.570842   7.280782   9.876851   7.163474  10.227235   0.889111  9.190222   \n",
       "1   3.006465  10.685368   5.047971  10.619064   9.339161   4.221861  0.595563   \n",
       "2  14.940933   4.579603  14.150860   1.819890  10.670237  10.350867  5.134417   \n",
       "3   4.999168  14.278131   0.650444   4.168135  12.782579   0.513072  0.321295   \n",
       "4   9.715296  14.142186   9.875013   7.908122   1.140498  14.165074  7.688796   \n",
       "\n",
       "          47         48         49         50         51         52  \n",
       "0   4.670908   0.931525   7.634177   0.158196  13.432551   2.511191  \n",
       "1  13.865748   0.328453  11.507599   9.107966   1.302407  11.105604  \n",
       "2   5.898995   8.374986   4.638049   3.160023   2.243799   5.073030  \n",
       "3  11.334062  11.735511   1.911520   8.365676   4.877288  11.601819  \n",
       "4   5.079241   8.152186   1.680403  13.215111   5.823109   1.038015  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = pd.read_csv(PATH_TEST)\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction we need only 2 features: **6 and 7**. More precisely, **6_square and 7**. And we can use coeffs and bias for calculating target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-bdd3733f4e91>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_preds_dataset['6_square'] = np.square(test_preds_dataset['6'])\n",
      "/home/diakap/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py:4305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>6_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218760</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.700228</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494556</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511117</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756416</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          7  6_square\n",
       "0  0.218760      11.0\n",
       "1  0.700228      79.0\n",
       "2  0.494556       7.0\n",
       "3  0.511117      38.0\n",
       "4  0.756416      64.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 2 our features from test\n",
    "preds_need = ['6', '7']\n",
    "test_preds_dataset = dataset_test[preds_need]\n",
    "test_preds_dataset['6_square'] = np.square(test_preds_dataset['6'])    \n",
    "\n",
    "# Delete unnessecary column\n",
    "test_preds_dataset.drop(columns=['6'], inplace=True)\n",
    "test_preds_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-0dc81c49d795>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_preds_dataset['PREDICTION'] = coeff[0] * test_preds_dataset['6_square'] + coeff[1] * test_preds_dataset['7'] + bias\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>6_square</th>\n",
       "      <th>PREDICTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218760</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.892505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.700228</td>\n",
       "      <td>79.0</td>\n",
       "      <td>78.219442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494556</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.935138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511117</td>\n",
       "      <td>38.0</td>\n",
       "      <td>37.626271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756416</td>\n",
       "      <td>64.0</td>\n",
       "      <td>63.369535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          7  6_square  PREDICTION\n",
       "0  0.218760      11.0   10.892505\n",
       "1  0.700228      79.0   78.219442\n",
       "2  0.494556       7.0    6.935138\n",
       "3  0.511117      38.0   37.626271\n",
       "4  0.756416      64.0   63.369535"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get target\n",
    "test_preds_dataset['PREDICTION'] = coeff[0] * test_preds_dataset['6_square'] + coeff[1] * test_preds_dataset['7'] + bias\n",
    "test_preds_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_dataset.to_csv('data_files/internship_test_PREDICTION.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
